{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Atlassian Data Center Helm Charts","text":"<p>This project contains Helm charts for installing Atlassian's Jira Data Center, Confluence Data Center, Bitbucket Data Center, Bamboo Data Center and Crowd Data Center on Kubernetes.</p> <p>Use the charts to install and operate Data Center products within a Kubernetes cluster of your choice. It can be a managed environment, such as Amazon EKS, Azure Kubernetes Service, Google Kubernetes Engine, or a custom on-premise system.</p>"},{"location":"#support-disclaimer","title":"Support disclaimer","text":"<p>Support Disclaimer</p> <p>Helm is a Kubernetes package manager that orchestrates the provisioning of applications onto existing Kubernetes infrastructure. The requirements for this infrastructure are described in Prerequisites. The Kubernetes cluster remains your responsibility; we do not provide direct support for Kubernetes or the underlying hardware it runs on.</p> <p>If you have followed our documentation on how to configure the Helm charts, and you're using correctly created components, we will then provide support if you encounter an error with installation after running the <code>helm install</code> command. </p> <p>We don\u2019t officially support the functionality described in the examples or the documented platforms. You should use them for reference only.</p> <p>Read more about what we support and what we don\u2019t. </p> <p>Certain product limitations listed below:</p> <ul> <li>Jira currently has limitations with scaling.</li> <li>Bamboo has a number of limitations, particularly with deployment and clustering.</li> </ul> <p>Read more about these product and platform limitations.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>The diagram below provides a high level overview of what a typical deployment might look like when using the Atlassian Data Center Helm charts:</p> <p></p>"},{"location":"#installing-the-helm-charts","title":"Installing the Helm charts","text":"<ul> <li>Prerequisites and setup - everything you need to do before installing the Helm charts</li> <li>Verification - verify the integrity of the Helm charts</li> <li>Installation - the steps to install the Helm charts</li> <li>Migration - what you have to do if you're migrating an existing deployment to Kubernetes</li> </ul>"},{"location":"#additional-content","title":"Additional content","text":"<ul> <li>Operation - how to upgrade applications, scale your cluster, and update resources</li> <li>Configuration - a deep dive into the configuration parameters</li> <li>Platforms support - how to allow support for different platforms</li> <li>Examples - various configuration examples</li> <li>Troubleshooting - how to debug issues with installation</li> </ul>"},{"location":"#product-versions","title":"Product versions","text":"<p>The minimum versions that we support for each product are:</p> Jira DC Confluence DC Bitbucket DC Bamboo DC Crowd DC 8.19 7.13 7.12 8.1 4.3"},{"location":"#feedback","title":"Feedback","text":"<p>If you find any issues, raise a ticket. If you have general feedback or questions regarding the charts, use Atlassian Community Kubernetes space.</p>"},{"location":"#contributions","title":"Contributions","text":"<p>Contributions are welcome. Find out how to contribute. </p>"},{"location":"#license","title":"License","text":"<p>Apache 2.0 licensed, see license file.</p>"},{"location":"examples/EXAMPLES/","title":"Available examples","text":"<p>Support disclaimer</p> <p>Use the examples we provide as reference only, we don\u2019t offer official support for them. </p>"},{"location":"examples/EXAMPLES/#pre-requisites","title":"Pre-requisites","text":""},{"location":"examples/EXAMPLES/#kubernetes-clusters","title":"Kubernetes clusters","text":"<p>See examples of provisioning Kubernetes clusters on cloud-based providers:</p> <ul> <li>Amazon EKS </li> <li>Google GKE</li> <li>Azure AKS</li> </ul>"},{"location":"examples/EXAMPLES/#ingress","title":"Ingress","text":"<ul> <li>See an example of provisioning an NGINX Ingress controller</li> </ul>"},{"location":"examples/EXAMPLES/#database","title":"Database","text":"<ul> <li>See an example of creating an Amazon RDS database instance</li> </ul>"},{"location":"examples/EXAMPLES/#storage","title":"Storage","text":"AWS EBSAWS EFSNFS <ul> <li>See an example of local storage utilizing AWS EBS-backed volumes</li> </ul> <ul> <li>See an example of shared storage utilizing AWS EFS-backed filesystem</li> </ul> <ul> <li>See an example of standing up an NFS server for Bitbucket</li> </ul>"},{"location":"examples/EXAMPLES/#bamboo","title":"Bamboo","text":""},{"location":"examples/EXAMPLES/#remote-agents","title":"Remote agents","text":"<ul> <li>See an example of deploying a remote agent for Bamboo</li> </ul>"},{"location":"examples/EXAMPLES/#bitbucket","title":"Bitbucket","text":""},{"location":"examples/EXAMPLES/#elasticsearch","title":"Elasticsearch","text":"<ul> <li>See an example of standing up an Elasticsearch instance for Bitbucket</li> </ul>"},{"location":"examples/EXAMPLES/#smart-mirrors","title":"Smart Mirrors","text":"<ul> <li>See an example of Bitbucket Smart Mirrors</li> </ul>"},{"location":"examples/EXAMPLES/#ssh","title":"SSH","text":"<ul> <li>See an example of SSH service in Bitbucket on Kubernetes</li> </ul>"},{"location":"examples/EXAMPLES/#mesh","title":"Mesh","text":"<ul> <li>See an example of Bitbucket Mesh on Kubernetes</li> </ul>"},{"location":"examples/EXAMPLES/#other","title":"Other","text":""},{"location":"examples/EXAMPLES/#logging","title":"Logging","text":"<ul> <li>See an example of how to deploy an EFK stack to Kubernetes</li> </ul>"},{"location":"examples/EXAMPLES/#customization","title":"Customization","text":"<ul> <li>See an example of External libraries and plugins</li> </ul>"},{"location":"examples/bamboo/AGENT_CAPABILITIES/","title":"Agent capabilities","text":"<p>A capability is a feature of an agent. A capability can be defined on an agent for:</p> <ul> <li>an executable (e.g. Maven)</li> <li>a JDK</li> <li>a Version Control System client application (e.g. Git)</li> </ul> <p>You can learn more about remote agents capabilities on the official documentation page.</p>"},{"location":"examples/bamboo/AGENT_CAPABILITIES/#custom-capabilities","title":"Custom capabilities","text":"<p>Default capabilities</p> <p>By default the Bamboo agent Helm chart will deploy the bamboo-agent-base Docker image. This image provides the following capabilities out of the box:</p> <ul> <li>JDK 11</li> <li>Git &amp; Git LFS</li> <li>Maven 3</li> <li>Python 3</li> </ul> <p>If additional capabilities are required, the Bamboo agent base Docker image can be extended with those capabilities. </p> <p>This custom image can be used, by first updating the Bamboo agent <code>values.yaml</code> with the image <code>tag</code> of the custom Docker image i.e.</p> <pre><code>image:\nrepository: hoolicorp/bamboo-agent-base\npullPolicy: IfNotPresent\ntag: \"ruby-agent\"\n</code></pre> <p>The custom agent can then be deployed via Helm:</p> <pre><code>helm install ruby-agent atlassian-data-center/bamboo-agent -f ruby-agent.yaml\n</code></pre>"},{"location":"examples/bamboo/REMOTE_AGENTS/","title":"Remote agents","text":"<p>Remote agents can be provisioned to a Kubernetes cluster to run jobs delegated to them via a Bamboo server. An agent can run a job if its capabilities match the requirements of a job. Each job inherits the requirements from individual tasks that it contains.</p> <p>You can learn more details about remote agents on the official documentation page.</p>"},{"location":"examples/bamboo/REMOTE_AGENTS/#requirements","title":"Requirements","text":"<p>Bamboo server prerequisites</p> <ul> <li>The Bamboo server instance must use a valid Bamboo Data Center instance license and be fully configured</li> <li>The Bamboo server instance must have <code>security token verification</code> enabled</li> <li>The Bamboo server instance must have <code>remote agent authentication</code> disabled</li> </ul>"},{"location":"examples/bamboo/REMOTE_AGENTS/#deployment","title":"Deployment","text":"<p>Steps required for deploying a remote agent</p> <ol> <li>Configure Bamboo server for remote agent support</li> <li>Deploy agent</li> </ol>"},{"location":"examples/bamboo/REMOTE_AGENTS/#1-configure-bamboo-server","title":"1. Configure Bamboo server","text":"<p>There are 2 approaches for doing this:</p> <ul> <li>Automatically when deploying Bamboo server</li> <li>Manually via Bamboo server agent settings</li> </ul>"},{"location":"examples/bamboo/REMOTE_AGENTS/#automatically","title":"Automatically","text":"<p>When initially deploying Bamboo server its <code>values.yaml</code> can be configured to:</p> <ul> <li>disable <code>remote agent authentication</code></li> <li>define a custom <code>security token</code> </li> </ul> <p>This will allow remote agents that are configured with the same security token to automatically join the cluster. </p> <p>First, create a secret to store a custom security token with which remote agent(s) should authenticate to the Bamboo server. </p> <p>Security token format</p> <p>The security token should be set to a 40-character hexadecimal string. The following command can be used to generate a string in this format: <pre><code>xxd -l 20 -p /dev/urandom\n</code></pre></p> <p>Add the generated string (security token) to a K8s secret</p> <pre><code>kubectl create secret generic security-token --from-literal=security-token=&lt;security token&gt;\n</code></pre> <p>Update the Bamboo <code>values.yaml</code> with this secret and disable agent authentication:</p> <pre><code>bamboo:\nsecurityToken:\nsecretName: \"security-token\"\nsecretKey: security-token\ndisableAgentAuth: true\n</code></pre> <p>Disabling remote agent authentication</p> <p>when setting the property <code>disableAgentAuth</code> to <code>true</code> this will have the effect of automatically allowing agents with the correct security token to communicate with the Bamboo server. This property is useful for testing, and when deployments requiring many agents are needed. This property can also be left in its default state of <code>false</code> in which case each agent will need to be approved manually via the <code>Agents</code> settings tab of the Bamboo server instance. Additional details on agent authentication can be found here </p>"},{"location":"examples/bamboo/REMOTE_AGENTS/#manually","title":"Manually","text":"<ul> <li> <p>When logged into the Bamboo server instance, and from the <code>Agents</code> settings tab, enable <code>security token verification</code>, and disable <code>remote agent authentication</code> </p> </li> <li> <p>Navigate to the remote agent's installation page by selecting the <code>Install remote agent</code> button from the <code>Agents</code> settings tab    </p> </li> <li> <p>Create a K8s secret using the <code>security token</code> rendered on the <code>Installing a remote agent</code> page    </p> </li> </ul> <p>create secret using token...</p> <pre><code>kubectl create secret generic security-token --from-literal=security-token=&lt;security token&gt;\n</code></pre>"},{"location":"examples/bamboo/REMOTE_AGENTS/#2-deploy-agent","title":"2. Deploy agent","text":"<ul> <li>Update the bamboo agent <code>values.yaml</code> to utilize the security token secret and point to the bamboo server instance</li> </ul> <pre><code>replicaCount: 3\nagent:\nsecurityToken:\nsecretName: \"security-token\"\nsecretKey: security-token\nserver: \"bamboo.bamboo.svc.cluster.local\"\n</code></pre> <p>Values</p> <ul> <li>As long as your cluster has the physical resources the <code>replicaCount</code> can be set to any value from <code>1</code> .. <code>1 + n</code> </li> <li><code>agent.server</code> should be configured with the K8s DNS record for the Bamboo server service. The value should be of the form: <code>&lt;service_name&gt;.&lt;namespace&gt;.svc.cluster.local</code></li> </ul> <ul> <li>Install the agent</li> </ul> <pre><code>helm install bamboo-agent atlassian-data-center/bamboo-agent -f values.yaml\n</code></pre> <p>Custom agents<p>By default the Bamboo agent Helm chart will deploy the bamboo-agent-base Docker image. This image provides the following capabilities out of the box:</p> <ul> <li>JDK 11</li> <li>Git &amp; Git LFS</li> <li>Maven 3</li> <li>Python 3</li> </ul> <p>For details on defining and deploying agents with custom/additional capabilities view the agent capabilities guide</p> </p>"},{"location":"examples/bamboo/REMOTE_AGENTS/#scaling-the-agent-count","title":"Scaling the agent count","text":"<p>The number of active agents can be easily increased or decreased: </p> <pre><code>helm upgrade --set replicaCount=&lt;desired number of agents&gt; \\\n--reuse-values \\\n&lt;name of the release&gt;\n             atlassian-data-center/bamboo-agent\n</code></pre>"},{"location":"examples/bamboo/REMOTE_AGENTS/#troubleshooting","title":"Troubleshooting","text":"<p>You can find the most common errors relating to agent configuration in the official Bamboo agent documentation.</p>"},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/","title":"Bitbucket Elasticsearch recommendations","text":"<p>While Bitbucket has its own internal Elasticsearch instance, we highly recommend you use an external Elasticsearch installation, either within the Kubernetes cluster or, if available, an instance managed by your hosting provider.</p>"},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#installing-and-configuring-elasticsearch-in-your-kubernetes-cluster","title":"Installing and configuring Elasticsearch in your Kubernetes cluster","text":""},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#installing-elasticsearch-into-your-kubernetes-cluster","title":"Installing Elasticsearch into your Kubernetes cluster","text":"<p>Choose a version of Elasticsearch that is supported by the version of Bitbucket you are installing. For Bitbucket 7.14 the latest supported Elasticsearch version is 7.9.3, so we will target that.</p> <p>There are official Helm charts for Elasticsearch 7.9.3. Following the documentation there add the Elasticsearch Helm charts repository:</p> <p><pre><code>helm repo add elastic https://helm.elastic.co\n</code></pre> then install it: <pre><code>helm install elasticsearch --namespace &lt;namespace&gt; --set imageTag=\"7.9.3\" elastic/elasticsearch\n</code></pre></p> <p>Prerequisites of Elasticsearch Helm chart</p> <p>Running the above commands will install Elasticsearch with the default configuration, which is 3 worker nodes.  However, it may not always work out of the box if failed to fulfill prerequisites for the default installation.  Some example prerequisites include:</p> <ul> <li>CPU/memory requests: 1000m/2Gi (for each worker node)</li> <li>Preconfigured storage volumes (30Gi for each worker node)</li> </ul> <p>For more details refer to Elasticsearch values.yaml file.</p>"},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#configuring-your-bitbucket-deployment","title":"Configuring your Bitbucket deployment","text":"<p>To enable the installed Elasticsearch service you need to configure the service URL under <code>bitbucket:</code> stanza in the <code>values.yaml</code> file. Check the Kubernetes official documentation on how to get DNS record for a service. <pre><code>bitbucket:\nelasticSearch:\nbaseUrl: http://elasticsearch-master.&lt;namespace&gt;.svc.cluster.local:9200\n</code></pre> This will also have the effect of disabling Bitbucket\u2019s internal Elasticsearch instance.</p> <p>Elasticsearch security</p> <p>If you have Elasticsearch cluster with security enabled, i.e. having credential details stored in a Kubernetes secret and passed into <code>extraEnvs</code> as this example does, you can then use the same secret and configure that in the bitbucket <code>values.yaml</code> file:      <pre><code>bitbucket:\nelasticSearch:    credentials:\nsecretName: &lt;my-elasticsearch-secret&gt;\nusernameSecretKey: username\npasswordSecretKey: password\n</code></pre> Read about Kubernetes secrets.</p>"},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#configuring-amazon-elasticsearch-service-with-bitbucket-on-kubernetes","title":"Configuring Amazon Elasticsearch Service with Bitbucket on Kubernetes","text":""},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#creating-an-amazon-elasticsearch-service-domain-with-a-master-user","title":"Creating an Amazon Elasticsearch Service domain with a master user","text":"<p>The Elasticsearch instance (\u201cdomain\u201d) can be created via the AWS CLI or the web console; for this example we will use the web console and a master user:</p> <ol> <li>In the EKS console navigate to Your Cluster \u2192 Networking and note the VPC ID.</li> <li>In the Elasticsearch console create a new domain:</li> <li>Select a production deployment.</li> <li>Select Elasticsearch version 7.9.</li> <li>In the next screen configure the AZs and nodes as appropriate for your expected workload.</li> <li>On the Access and security page:</li> <li>Select the same VPC as the EKS cluster, as noted in step 1.</li> <li>Select appropriate subnets for each AZ; private subnets are fine.</li> <li>Select appropriate security groups that will grant node/pod access.</li> <li>Tick Fine\u2013grained access control:<ul> <li>Select Create master user and add a username and a strong password.</li> </ul> </li> <li>Configure tags, etc. as appropriate for your organisation.</li> </ol> <p>Once the Elasticsearch domain has finished creating, make a note of the VPC Endpoint, which will be an HTTPS URL.</p>"},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#configuring-your-bitbucket-deployment_1","title":"Configuring your Bitbucket deployment","text":"<p>To use the managed Elasticsearch service, first create a Kubernetes secret using the username and password from step 4 above. Then configure the service URL under <code>bitbucket:</code> in the <code>values.yaml</code> file, substituting the values below from the above steps where appropriate: <pre><code>bitbucket:\nelasticSearch:\nbaseUrl: &lt;VPC Endpoint&gt;\ncredentials:\nsecretName: &lt;my-elasticsearch-secret&gt;\nusernameSecretKey: username\npasswordSecretKey: password\n</code></pre></p> <p>Read about Kubernetes secrets.</p>"},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#testing-your-elasticsearch-connection","title":"Testing your Elasticsearch connection","text":"<p>To test if Elasticsearch is properly set up, go to Administration &gt; System - Server settings. The Elasticsearch URL should be pre-populated already in the search section. Click the Test button to see if it connects successfully. </p>"},{"location":"examples/bitbucket/BITBUCKET_MESH/","title":"Bitbucket Mesh","text":"<p>Bitbucket Mesh is a distributed, replicated, and horizontally scalable Git repository storage system, which increases performance and improves the resilience of Bitbucket.</p> <p></p> <p>You can learn more details about Bitbucket Mesh on the official documentation page.</p> <p>Recommendations for Mesh deployments</p> <p>Bitbucket version</p> <p>By default, the Helm charts target the latest Bitbucket LTS version. However, Mesh is only supported from version 8.0. You will need to select an 8.x version of Bitbucket to deploy Mesh. Learn more details below.</p> <p>Mesh agent version</p> <p>Bitbucket Mesh agents are versioned independently from Bitbucket. You should select the appropriate version for the deployed version of Bitbucket. See the Mesh download page for available versions.</p> <p>Number of Mesh nodes</p> <p>In order for high-availability to be possible, we recommend having a minimum of three Mesh nodes. There is no maximum on the number of nodes.</p> <p>Mesh node co-location</p> <p>We don't currently support deploying Mesh nodes into multiple availability zones. Just like the shared file system based deployments, the Mesh nodes (that is, the repository storage) and the application nodes must be co-located.</p> <p>Other Mesh deployment requirements</p> <p>For more details on the requirements and limitations of Mesh deployments, check the the Bitbucket Mesh FAQ.</p>"},{"location":"examples/bitbucket/BITBUCKET_MESH/#configuring-your-bitbucket-and-mesh-deployment","title":"Configuring your Bitbucket and Mesh deployment","text":"<p>For backwards compatibility, the Helm charts default to Mesh being disabled. To enable it, you will need to configure the service under <code>bitbucket:</code> stanza in the <code>values.yaml</code> file, substituting the values below from the above steps where appropriate:</p> <pre><code>image:\ntag: &lt;an 8.x.x version of Bitbucket&gt;\n\nbitbucket:\nmesh:\nenabled: true\nimage:\nversion: &lt;Mesh agent version&gt;\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_MESH/#adding-the-mesh-nodes-to-bitbucket","title":"Adding the Mesh nodes to Bitbucket","text":"<p>To enable the deployed Mesh nodes you need to add them to the Bitbucket Data Center instance in the administration area. To do so, you'll need the service URL of each node; these are usually of the form <code>bitbucket-mesh-&lt;num&gt;</code>. Check the Kubernetes official documentation to learn how to get a DNS record for a service.</p> <p>To connect the Mesh node:</p> <ol> <li>In your Bitbucket Data Center instance, navigate to Administration &gt; Git &gt; Bitbucket Mesh.</li> <li>Enter the URL of the Mesh node in the Node URL field (e.g. <code>http://bitbucket-mesh-1:7777</code>).</li> <li>(Optional) Enter a name for the Mesh node in the Node name field.</li> <li>Select Add Mesh node.</li> </ol> <p>Learn more details about Mesh configuration.</p>"},{"location":"examples/bitbucket/BITBUCKET_MESH/#migrating-existing-repositories-to-mesh","title":"Migrating existing repositories to Mesh","text":"<p>Learn how to migrate repositories to Mesh.</p>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/","title":"Smart Mirroring","text":"<p>Smart Mirroring can greatly improve Git clone speeds for distributed teams working with large repositories. Large repositories that take hours to clone from a Bitbucket instance over the Internet from the other side of the world can take minutes when cloned from a local mirror on a fast network.</p> <p></p> <p>You can learn more details about smart mirroring on the official documentation page.</p> <p>Upstream/Primary instance</p> <p>Primary instance is sometimes called upstream instance.</p>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#requirements","title":"Requirements","text":"<p>Primary instance prerequisites</p> <p>Your primary Bitbucket instance must be a fully licensed Bitbucket Data Center instance</p> <p>You do not have to run your Bitbucket Data Center instance as a multi-node cluster to use smart mirroring, but you must have an up-to-date Data Center license.</p> <p>The primary instance and all mirror(s) must have HTTPS with a valid (i.e., signed by a Certificate Authority anchored to the root and not expired) SSL certificate</p> <p>This is a strict requirement of smart mirroring on both the primary instance and all mirror(s), and cannot be bypassed. The mirror setup wizard will not proceed if either the mirror or the primary instance does not have a valid SSL certificate.</p> <p>The primary Bitbucket instance must have SSH enabled</p> <p>Mirrors keep their repositories synchronized with the primary instance over SSH and cannot use HTTP or HTTPS for this. See Enable SSH access to Git repositories for instructions on enabling SSH access on your primary instance.</p>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#installation","title":"Installation","text":""},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#overview","title":"Overview","text":"<ol> <li>Install the primary as usual with Bitbucket Helm chart.<ul> <li>You need to make sure the instance complies with all the above listed requirements.</li> </ul> </li> <li>Install the mirror with second Bitbucket Helm chart.<ul> <li>There is a set of properties that need to be configured to make the mirror work.</li> </ul> </li> <li>Approve the mirror in the primary instance.</li> </ol>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#steps","title":"Steps","text":""},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#mirror-farm-installation-in-kubernetes","title":"Mirror Farm installation in Kubernetes","text":"<p>Info</p> <p>Example is using nginx-ingress controller. If you are using a different ingress controller, you will need to modify the example.</p> <ol> <li>Install the primary as usual with Bitbucket Helm chart.<ul> <li>You need to make sure the instance complies with all the above listed requirements.</li> <li>Verify that you are able to clone from the primary via SSH protocol.</li> <li>Verify that primary instance is accessible over HTTPS with a valid SSL certificate.</li> </ul> </li> <li>Create a new file <code>values-mirror.yaml</code> with the following content: <pre><code>bitbucket:\nmode: mirror\ndisplayName: Bitbucket Mirror\nclustering:\nenabled: true\napplicationMode: \"mirror\"\nmirror:\nupstreamUrl: &lt;https url of the primary&gt; # for example https://bitbucket-upstream.example.com\n\n# nginx specific configuration\ningress:\ncreate: true\nhost: bitbucket-mirror.example.com\nannotations:\ncert-manager.io/issuer: \"letsencrypt-prod\" # Default issuer\ntlsSecretName: &lt;secret with TLS private key&gt; # E.g. tls-certificate-mirror\n\n# enables persistence for mirror data\nvolumes:\nlocalHome:\npersistentVolumeClaim:\ncreate: true\n</code></pre></li> <li>Edit the file to change the placeholder values.<ul> <li><code>bitbucket.mirror.upstreamUrl</code></li> <li><code>ingress.host</code></li> <li><code>ingress.tlsSecretName</code></li> </ul> </li> <li>Install the mirror     <pre><code>  helm install bitbucket-mirror atlassian-data-center/bitbucket -f values-mirror.yaml\n</code></pre></li> </ol>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#mirror-farm-authorization","title":"Mirror farm authorization","text":"<p>These steps are described in detail in official documentation.</p> <ol> <li>Visit the mirror URL (it might take a couple of minutes to come up)</li> <li>Click on Go to the primary server in the mirror UI. The link will take you to the administration section of the primary instance. </li> <li>Click Authorize next to the mirror </li> <li>Select which projects should be synchronized </li> <li>Wait for the projects to be synchronized </li> <li>Verify that the synchronized projects can be cloned from the mirror </li> </ol>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#scaling-the-mirror-farm","title":"Scaling the mirror farm","text":"<p>As the mirror is deployed with all the fulfilled requirements for the Bitbucket Mirror Farm, you are able to scale the mirrors easily. To increase or decrease the size of the mirror farm:</p> <pre><code>helm upgrade --set replicaCount=&lt;desired number of mirror nodes&gt; \\\n--reuse-values \\\n&lt;name of the release&gt;\n             atlassian-data-center/bitbucket\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#adding-or-removing-additional-mirror-farms","title":"Adding or removing additional mirror farms","text":"<p>It is possible to connect multiple mirror farms to a single primary instance. This can be useful to improve local performance for geographically distributed teams.</p> <p>To add a new mirror farm, follow the same steps that were necessary to connect the first mirror farm. This means installing another helm release and authenticating it in the administrator user interface.</p> <p>To remove a mirror farm:</p> <ol> <li>Navigate to Mirrors administration section on the primary</li> <li>Select the mirror from the list</li> <li>Click Delete button</li> <li>Uninstall the deleted mirror helm release from the cluster</li> </ol>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#troubleshooting","title":"Troubleshooting","text":"<p>You can find the most common errors in mirror configuration described in the official Bitbucket documentation.</p>"},{"location":"examples/bitbucket/BITBUCKET_SSH/","title":"SSH service in Bitbucket on Kubernetes","text":"<p>In addition to providing a service on HTTP(S), Bitbucket also allows remote Git operations over SSH connections. By default, Kubernetes Ingress controllers only work for HTTP connections, but some ingress controllers also support TCP connections.</p> <p>Depending on the need of your deployment, SSH access can be provided through two mechanisms:</p> <ol> <li>Opening the TCP port through the ingress controller - This option should be used if the SSH service is required to be available on the same DNS name as the HTTP service.</li> <li>Creating a separate Kubernetes <code>LoadBalancer</code> service - This option is available if the ingress controller does not support TCP connections, or if you don\u2019t need your deployment to have the SSH service available on the same DNS name as the HTTP service.</li> </ol>"},{"location":"examples/bitbucket/BITBUCKET_SSH/#nginx-ingress-controller-config-for-ssh-connections","title":"NGINX Ingress controller config for SSH connections","text":"<p>We can follow the official documentation for the NGINX Ingress controller for this: Exposing TCP and UDP services - NGINX Ingress Controller.</p> <p>Namespace co-location</p> <p>These instructions should be performed in the same namespace in which the Ingress controller resides.</p>"},{"location":"examples/bitbucket/BITBUCKET_SSH/#1-create-configmap","title":"1. Create ConfigMap","text":"<p>Create a new <code>ConfigMap</code>: <pre><code>kubectl create configmap tcp-services\n</code></pre></p> <p>In our example we deployed Bitbucket using the Helm release name <code>bitbucket</code> in the namespace <code>ssh-test</code>, update the <code>ConfigMap</code> <code>tcp-services</code> accordingly:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: tcp-services\nnamespace: ingress-nginx\ndata:\n7999: \"&lt;bitbucket namespace&gt;/&lt;bitbucket helm release name&gt;:ssh\"\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_SSH/#2-update-ingress-deployment","title":"2. Update Ingress deployment","text":"<p>Next, we have to edit the <code>deployment</code> of the ingress controller and add the <code>--tcp-services-configmap</code> option: <pre><code>kubectl edit deployment &lt;name of ingress-nginx deployment&gt;\n</code></pre> Add this line in the <code>args</code> of the container <code>spec</code>: <pre><code>- --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n</code></pre> so it looks something like this: <pre><code> spec:\ncontainers:\n- args:\n- /nginx-ingress-controller\n- --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n- --election-id=ingress-controller-leader\n- --ingress-class=nginx\n- --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n- --validating-webhook=:8443\n- --validating-webhook-certificate=/usr/local/certificates/cert\n- --validating-webhook-key=/usr/local/certificates/key\n- --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n</code></pre></p>"},{"location":"examples/bitbucket/BITBUCKET_SSH/#3-update-the-ingress-service","title":"3. Update the Ingress service","text":"<p>Update the Ingress service to include an additional <code>port</code> definition for <code>ssh</code> <pre><code>kubectl edit service &lt;name of ingress-nginx service&gt;\n</code></pre> Add this section in the <code>ports</code> of the container <code>spec</code>: <pre><code>- name: ssh\nport: 7999\nprotocol: TCP\n</code></pre> so it looks something like this: <pre><code>spec:\nclusterIP: 10.100.19.60\nexternalTrafficPolicy: Cluster\nports:\n- name: http\nnodePort: 31381\nport: 80\nprotocol: TCP\ntargetPort: http\n- name: https\nnodePort: 32612\nport: 443\nprotocol: TCP\ntargetPort: https\n- name: ssh\nport: 7999\nprotocol: TCP\n</code></pre> After the deployment has been upgraded, the <code>SSH</code> service should be available on port <code>7999</code>.</p>"},{"location":"examples/bitbucket/BITBUCKET_SSH/#loadbalancer-service-for-ssh-connections-on-aws","title":"LoadBalancer service for SSH connections on AWS","text":"<p>In the values file for the helm chart, the extra SSH service can be enabled like this: <pre><code>bitbucket:\nsshService:\nenabled: true\n</code></pre> On a deployment using AWS, assuming you have external-dns configured, you can add these annotations to automatically set up the DNS name for the SSH service: <pre><code>bitbucket:\nsshService:\nenabled: true\nannotations:\nexternal-dns.alpha.kubernetes.io/hostname: bitbucket-ssh.example.com\nadditionalEnvironmentVariables:\n- name: PLUGIN_SSH_BASEURL\nvalue: ssh://bitbucket-ssh.example.com/\n</code></pre></p>"},{"location":"examples/cluster/AKS_SETUP/","title":"Preparing an AKS cluster","text":"<p>This example provides instructions for creating a Kubernetes cluster using Azure AKS.</p>"},{"location":"examples/cluster/AKS_SETUP/#prerequisites","title":"Prerequisites","text":"<p>We recommend installing and configuring the Azure Cloud Shell, allowing for CLI interaction with the AKS cluster.</p>"},{"location":"examples/cluster/AKS_SETUP/#manual-creation","title":"Manual creation","text":"<p>Follow the Azure Kubernetes Service Quickstart for details on creating an AKS cluster.</p> <p>Next step - Ingress controller</p> <p>Having established a cluster, continue with provisioning the next piece of prerequisite infrastructure, the Ingress controller.</p>"},{"location":"examples/cluster/CLOUD_PROVIDERS/","title":"Provisioning Kubernetes clusters on cloud-based providers","text":"<p>Here are installation and configuration instructions for cloud providers:</p> <ul> <li>Amazon EKS </li> <li>Google GKE</li> <li>Azure AKS</li> </ul>"},{"location":"examples/cluster/EKS_SETUP/","title":"Preparing an EKS cluster","text":"<p>This example provides instructions for creating a Kubernetes cluster using Amazon EKS.</p>"},{"location":"examples/cluster/EKS_SETUP/#prerequisites","title":"Prerequisites","text":"<p>We recommend installing and configuring eksctl, allowing for CLI interaction with the EKS cluster.</p>"},{"location":"examples/cluster/EKS_SETUP/#manual-creation","title":"Manual creation","text":"<p>Follow the Getting started with Amazon EKS for details on creating an EKS cluster. Or, using the <code>ClusterConfig</code> below as an example, deploy a K8s cluster with <code>eksctl</code> in ~20 minutes:</p> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\nname: atlassian-cluster\nregion: ap-southeast-2\n\nmanagedNodeGroups:\n- name: appNodes\ninstanceType: m5.large\ndesiredCapacity: 2\nssh: # enable SSH using SSM\nenableSsm: true\n</code></pre> Cluster considerations <p>It's always a good idea to consider the following points before creating the cluster:</p> <ol> <li>Geographical region - where will the cluster reside.</li> <li>EC2 instance type - the instance type to be used for the nodes that make up the cluster.</li> <li>Number of nodes - guidance on the resource dimensions that should be used for these nodes can be found in Requests and limits.</li> </ol> <p>Adding the config above to a file named <code>config.yaml</code> provision the cluster: </p> <pre><code>eksctl create cluster -f config.yaml\n</code></pre> <p>Next step - Ingress controller</p> <p>Having established a cluster, continue with provisioning the next piece of prerequisite infrastructure, the Ingress controller.</p>"},{"location":"examples/cluster/GKE_SETUP/","title":"Preparing an GKE cluster","text":"<p>This example provides instructions for creating a Kubernetes cluster using Google GKE.</p>"},{"location":"examples/cluster/GKE_SETUP/#prerequisites","title":"Prerequisites","text":"<p>We recommend installing and configuring Google Cloud SDK, allowing for CLI interaction with an GKE cluster.</p>"},{"location":"examples/cluster/GKE_SETUP/#manual-creation","title":"Manual creation","text":"<p>Follow the How-to guides for details on creating an GKE cluster. </p> <p>Next step - Ingress controller</p> <p>Having established a cluster, continue with provisioning the next piece of prerequisite infrastructure, the Ingress controller.</p>"},{"location":"examples/database/AMAZON_RDS/","title":"Creating an RDS database instance","text":"<p>This example provides instructions for creating an Amazon RDS DB instance.</p>"},{"location":"examples/database/AMAZON_RDS/#prerequisites","title":"Prerequisites","text":"<ul> <li>An <code>AWS account</code>, <code>IAM user</code>, <code>VPC</code> (or default VPC) and <code>security group</code> are required before an RDS DB instance can be created. See Setting up for Amazon RDS for further instructions.</li> </ul>"},{"location":"examples/database/AMAZON_RDS/#database-creation","title":"Database creation","text":"<p>There are two steps for creating the database:</p> <ol> <li>Initialize database server</li> <li>Initialize database and user</li> </ol>"},{"location":"examples/database/AMAZON_RDS/#1-initialize-database-server","title":"1. Initialize database server","text":"<p>For details on standing up an RDS DB server follow the guide: Creating an Amazon RDS DB instance.</p>"},{"location":"examples/database/AMAZON_RDS/#2-initialize-database-and-user","title":"2. Initialize database and user","text":"<p>Don't forget to create the database and user!</p> <p>This is a required step. For details on creating the application database and database user follow the appropriate guide below:</p> JiraConfluenceBitbucketBambooCrowd <p>Create database for Jira</p> <p>Create database for Confluence</p> <p>Create database for Bitbucket</p> <p>Create database for Bamboo</p> <p>Create database for Crowd</p> <p>Next step - Shared storage</p> <p>Having created the database continue with provisioning the next piece of prerequisite infrastructure, shared storage.</p>"},{"location":"examples/database/CLOUD_PROVIDERS/","title":"Provisioning databases on cloud-based providers","text":"<p>Supported databases</p> <p>Your selected database engine type must be supported by the Data Center product you wish to install:</p> JiraConfluenceBitbucketBambooCrowd <p>Jira supported databases</p> <p>Confluence supported databases</p> <p>Bitbucket supported databases</p> <p>Supported databases</p> <p>Crowd supported databases</p> <p>Database deployment and configuration instructions for cloud providers can be found below:</p> <ul> <li>Amazon RDS</li> </ul>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/","title":"External libraries and plugins","text":"<p><code>.jar</code> files only</p> <p>Whether loading external libraries, drivers or plugins, the approaches outlined here can only be used with <code>.jar</code> files. Plugin <code>obr</code> files can be extracted (unzipped) to access the included <code>.jar</code></p> <p>In some situations, you may want to load 3rd party plugins, drivers or libraries so that they are available to the product  being installed.</p> <p>An example of when this may be needed are for those products that do not ship with the appropriate <code>MySQL</code> and <code>Oracle</code> <code>JDBC</code> drivers.</p> <p>There are 3 strategies for doing this, you can either:</p> <ul> <li>use the required prerequisite shared home volume </li> <li>create a custom volume specifically for this purpose </li> <li>provide a custom command for the <code>nfsPermissionFixer</code></li> </ul> <p>Each approach will be discussed below.</p> <p>Approach</p> <p>Which approach is used is totally up to you. For convenience you may want to just use shared-home, or if you'd like to  keep things clean you may decide to mount these 3rd party libraries in a volume of their own. This approach would be  particularly useful when these libraries need to be shared with other Pod's in your cluster.</p>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#shared-home-volume","title":"Shared home volume","text":"<p>This approach consists of 3 high-level tasks:</p> <ol> <li>Create sub-dir in <code>shared-home</code> volume</li> <li>Copy libraries to sub-dir</li> <li>Update <code>additionalLibraries</code> stanza in <code>values.yaml</code></li> </ol>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#1-create-sub-dir","title":"1. Create sub-dir","text":"<p>Add the Pod definition below to a file called <code>shared-home-browser.yaml</code> </p> <p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: shared-home-browser\nspec:\ncontainers:\n- name: browser\nimage: debian:stable-slim\nvolumeMounts:\n- mountPath: /shared-home\nname: shared-home\ncommand: [ \"bash\", \"-c\", \"--\" ]\nargs: [ \"while true; do sleep 30; done;\" ]\nvolumes:\n- name: shared-home\npersistentVolumeClaim:\nclaimName: &lt;shared-home-pvc-name&gt;\n</code></pre> Initialise the Pod in the same namespace in which the <code>shared-home</code> PVC was created <pre><code>kubectl apply -f shared-home-browser.yaml\n</code></pre> Once running execute the following command, it will create the sub-sir, <code>libraries</code>, under <code>/shared-home</code> <pre><code>kubectl exec -it shared-home-browser -- bash -c \"mkdir -p /shared-home/libraries\"\n</code></pre></p>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#2-copy-libraries-to-sub-dir","title":"2. Copy libraries to sub-dir","text":"<p>Now copy the files you require to the sub-dir by using the <code>kubectl cp</code> command <pre><code>kubectl cp my_library.jar shared-home-browser:/shared-home/libraries\n</code></pre></p>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#3-update-valuesyaml","title":"3. Update <code>values.yaml</code>","text":"<p>Update the stanza, <code>additionalLibraries</code>, in <code>values.yaml</code> accordingly: <pre><code>jira:\nadditionalLibraries:\n- volumeName: shared-home\nsubDirectory: libraries\nfileName: my_library.jar\n</code></pre> With this config these files (<code>my_library.jar</code>) will be injected into the container directory <code>&lt;product-installation-directory&gt;/lib</code>. For more info on how these files are injected into the appropriate product container location, see Jira's helper jira.additionalLibraries.  </p>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#custom-volume","title":"Custom volume","text":"<p>This approach is very similar to the Shared home volume approach, only a custom volume is created and used as opposed <code>shared-home</code>. </p> <ol> <li>Create a new volume for storing 3rd party libraries</li> <li>Create sub-dir for the new volume</li> <li>Copy libraries to sub-dir</li> <li>Update <code>additionalLibraries</code> stanza in <code>values.yaml</code></li> <li>Update <code>additionalVolumeMounts</code> stanza in <code>values.yaml</code></li> <li>Update <code>additional</code> stanza in <code>values.yaml</code></li> </ol> <p>Steps</p> <p>Because many of the steps for this approach are similar to the steps used for Shared home volume only those that differ will be discussed.</p>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#1-create-new-volume","title":"1. Create new volume","text":"<p>Using the same approach taken for provisioning the shared-home volume, create a new <code>EFS</code> with a corresponding <code>PV</code> and <code>PVC</code>.</p> <p>ReadOnlyMany</p> <p>Ensure that the PV and PVC are setup with <code>ReadOnlyMany</code> access</p>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#2-update-valuesyaml","title":"2. Update <code>values.yaml</code>","text":"<p>Assuming that the <code>PVC</code> representing the <code>EFS</code> is called <code>third-party-libraries</code>, update the <code>values.yaml</code> so that the <code>PVC</code> is added as an <code>additional</code> mount: <pre><code>volumes:\nadditional:\n- name: third-party-libraries\npersistentVolumeClaim:\nclaimName: third-party-libraries\n</code></pre> Now add this as an <code>additionalVolumeMounts</code> <pre><code>additionalVolumeMounts:\n- volumeName: third-party-libraries\nmountPath: /libraries\n</code></pre> Finally inject the desired libraries by defining them under <code>additionalLibraries</code> <pre><code>additionalLibraries:\n- volumeName: third-party-libraries\nsubDirectory: database_drivers\nfileName: my_library.jar\n</code></pre></p>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#custom-command","title":"Custom command","text":"<p>This example is based on the GitHub issue discussed here. The <code>nfsPermissionFixer</code> in the <code>values.yaml</code> is used for appropriately setting the permissions on the <code>shared-home</code> volume. The command it uses for this is already defined by default, however it can also be supplied with a custom <code>command</code> for adding 3rd party libraries to <code>shared-home</code>. The example below shows how this approach can be used for adding the <code>JDBC</code> <code>MySQL</code> driver:</p> <pre><code>nfsPermissionFixer:\ncommand: |\nif [[ ! -f /shared-home/drivers/mysql-driver.jar ]]; then\nmkdir -p /shared-home/drivers\napk add dpkg\nwget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java_8.0.26-1debian10_all.deb\ndpkg-deb -R mysql-connector-java_8.0.26-1debian10_all.deb /tmp/\ncp /tmp/usr/share/java/mysql-connector-java-8.0.26.jar  /shared-home/drivers/mysql-driver.jar\nfi\nchgrp 2003 /shared-home; chmod g+w /shared-home\n</code></pre> <p>shared-home permissions</p> <p>If taking this approach ensure the last thing your custom command does is apply the relevant permissions to the <code>shared-home</code> mount, see line <code>10</code> in <code>yaml</code>  snippet above. </p> <p>Each product chart has a <code>sharedHome.permissionFix.command</code> helper for doing this. look at Jira's helper sharedHome.permissionFix.command  for more details on how these permissions are applied by default.</p> <p>Remember to also update the <code>additionalLibraries</code> stanza accordingly: <pre><code>additionalLibraries: - volumeName: shared-home\nsubDirectory: drivers\nfileName: mysql-driver.jar\n</code></pre></p>"},{"location":"examples/ingress/CONTROLLERS/","title":"Provisioning an Ingress controller","text":"<p>In order for the provided Ingress resource to work, your Kubernetes cluster must have an ingress controller running. The Atlassian Helm charts have been tested with the NGINX Ingress Controller, however alternatives can also be used. </p> <p>Here is an example of how these controllers can be installed and configured for use with the Atlassian Helm charts:</p> <ul> <li>NGINX Ingress Controller</li> </ul>"},{"location":"examples/ingress/DNS/","title":"Create DNS record via AWS CLI","text":"<p>DNS record creation using Route53</p> <p>The approach below shows how a DNS record can be created using AWS Route53 and the AWS CLI for record sets</p> <p>First, identify the name of the auto provisioned AWS Classic Load Balancer that was created above for Step 2. Install controller: <pre><code>kubectl get service -n ingress | grep ingress-nginx | awk '{print $4}' | head -1\n</code></pre> the output of this command should be the name of the load balancer, take note of the name i.e. <pre><code>b834z142d8118406795a34df35e10b17-38927090.eu-west-1.elb.amazonaws.com\n</code></pre> Next, using the first part of the load balancer name, get the <code>HostedZoneId</code> for the load balancer <pre><code>aws elb describe-load-balancers --load-balancer-name b834z142d8118406795a34df35e10b17 --region &lt;aws_region&gt; | jq '.LoadBalancerDescriptions[] | .CanonicalHostedZoneNameID'\n</code></pre> With the <code>HostedZoneId</code> and the full name of the load balancer create the <code>JSON</code> \"change batch\" file below:</p> <pre><code>{\n\"Comment\": \"An alias resource record for Jira in K8s\",\n\"Changes\": [\n{\n\"Action\": \"CREATE\",\n\"ResourceRecordSet\": {\n\"Name\": &lt;DNS record name&gt;,\n\"Type\": \"A\",\n\"AliasTarget\": {\n\"HostedZoneId\": &lt;Load balancer hosted zone ID&gt;,\n\"DNSName\": &lt;Load balancer name&gt;,\n\"EvaluateTargetHealth\": true\n}\n}\n}\n]\n}\n</code></pre> <p>DNS record name</p> <p>If for example, the DNS record name were set to <code>product.k8s.hoolicorp.com</code> then the host, <code>hoolicorp.com</code>, would be the pre-registerd AWS Route53 hosted zone.</p> <p>Next get the zone ID for the hosted zone: <pre><code>aws route53 list-hosted-zones-by-name | jq '.HostedZones[] | select(.Name == \"hoolicorp.com.\") | .Id'\n</code></pre> Finally, using the hosted zone ID and the <code>JSON</code> change batch file created above, initialize the record: <pre><code>aws route53 change-resource-record-sets --hosted-zone-id &lt;hosted zone ID&gt; --change-batch file://change-batch.json\n</code></pre> This will return a response similar to the one below: <pre><code>{\n\"ChangeInfo\": {\n\"Id\": \"/change/C03268442VMV922ROD1M4\",\n\"Status\": \"PENDING\",\n\"SubmittedAt\": \"2021-08-30T01:42:23.478Z\",\n\"Comment\": \"An alias resource record for Jira in K8s\"\n}\n}\n</code></pre> You can get the current status of the record's initialization: <pre><code>aws route53  get-change --id /change/C03268442VMV922ROD1M4\n</code></pre> Once the <code>Status</code> has transitioned to <code>INSYNC</code> the record is ready for use... <pre><code>{\n\"ChangeInfo\": {\n\"Id\": \"/change/C03268442VMV922ROD1M4\",\n\"Status\": \"INSYNC\",\n\"SubmittedAt\": \"2021-08-30T01:42:23.478Z\",\n\"Comment\": \"Creating Alias resource record sets in Route 53\"\n}\n}\n</code></pre></p>"},{"location":"examples/ingress/INGRESS_NGINX/","title":"NGINX Ingress Controller - with TLS termination","text":"<p>NGINX ingress controller with automatic TLS certificate management using cert-manager and certificates from Let's Encrypt.</p> <p>Using these instructions</p> <p>These instructions are for reference purposes, as such they should be used for development and testing purposes only! See the official instructions for Deploying and configuring the controller.</p> <p>These instructions are composed of 3 high-level parts:</p> <ol> <li>Controller installation and configuration</li> <li>Certificate manager installation and configuration</li> <li>Ingress resource configuration</li> </ol>"},{"location":"examples/ingress/INGRESS_NGINX/#controller-installation-and-configuration","title":"Controller installation and configuration","text":"<p>We recommend installing the controller using its official Helm Charts. You can also use the instructions below.</p>"},{"location":"examples/ingress/INGRESS_NGINX/#1-add-controller-repository","title":"1. Add controller repository","text":"<p>Add the <code>ingress-nginx</code> Helm repository: <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n</code></pre> Update the repository: <pre><code>helm repo update\n</code></pre></p>"},{"location":"examples/ingress/INGRESS_NGINX/#2-install-controller","title":"2. Install controller","text":"<p>Create a new namespace for the Ingress controller: <pre><code>kubectl create namespace ingress\n</code></pre> Install the controller using Helm: <pre><code>helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress\n</code></pre></p> <p>This will take couple of minutes.</p> <p>Confirm your ingress controller is installed:</p> <pre><code>kubectl get pods --namespace ingress\n</code></pre>"},{"location":"examples/ingress/INGRESS_NGINX/#3-dns-setup","title":"3. DNS setup","text":"<p>Manually provision a new DNS record via your cloud provider, for instance AWS and Route53, or dynamically using external-dns. There are also instructions on how this can be done using the AWS CLI.</p> <p>Once created, associate the DNS record with the auto provisioned load balancer that was created in Step 2. above. To do this first identify the name of the auto provisioned LB, this can be done by examining the deployed ingress services i.e.: <pre><code>kubectl get service -n ingress | grep ingress-nginx\n</code></pre> the output of this command should look something like... <pre><code>ingress-nginx-controller             LoadBalancer   10.100.22.16    b834z142d8118406795a34df35e10b17-38927090.eu-west-1.elb.amazonaws.com   80:32615/TCP,443:31787/TCP   76m\ningress-nginx-controller-admission   ClusterIP      10.100.5.36     &lt;none&gt;                                                                  443/TCP                      76m\n</code></pre> Take note of the <code>LoadBalancer</code> and using it as a value update the DNS record so that traffic is routed to it.</p> <p>It can take a few minutes for the DNS to resolve these changes.</p>"},{"location":"examples/ingress/INGRESS_NGINX/#certificate-manager-installation-and-configuration","title":"Certificate manager installation and configuration","text":"<p>Kubernetes certificate management is handled using cert-manager.</p>"},{"location":"examples/ingress/INGRESS_NGINX/#1-install-cert-manager","title":"1. Install cert-manager","text":"<p>Add the cert-manager repository <pre><code>helm repo add jetstack https://charts.jetstack.io\n</code></pre></p> <p>Update repositories <pre><code>helm repo update\n</code></pre></p> <p>Install the cert-manager using Helm <pre><code>helm install \\\ncert-manager jetstack/cert-manager \\\n--namespace cert-manager \\\n--create-namespace \\\n--version v1.3.1 \\\n--set installCRDs=true\n</code></pre></p> <p>Confirm the cert-manager is appropriately installed: <pre><code>kubectl get pods --namespace cert-manager\n</code></pre></p>"},{"location":"examples/ingress/INGRESS_NGINX/#2-create-certificate-issuer","title":"2. Create certificate issuer","text":"<p>Using the <code>yaml</code> specification below create and apply the certificate <code>Issuer</code> resource:</p> <p>Namespace co-location</p> <p>Ensure that the certificate issuer is installed in the same namespace that the Atlassian product will be deployed to.</p> <p><pre><code>apiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\nname: letsencrypt-prod\nnamespace: &lt;product_deployment_namespace&gt;\nspec:\nacme:\n# The ACME server URL\nserver: https://acme-v02.api.letsencrypt.org/directory\n# Email address used for ACME registration\nemail: &lt;user_email&gt;\n# Name of a secret used to store the ACME account private key\nprivateKeySecretRef:\nname: letsencrypt-prod\n# Enable the HTTP-01 challenge provider\nsolvers:\n- http01:\ningress:\nclass: nginx\n</code></pre> Install the <code>Issuer</code> resource: <pre><code>kubectl apply -f issuer.yaml\n</code></pre></p>"},{"location":"examples/ingress/INGRESS_NGINX/#ingress-resource-configuration","title":"Ingress resource configuration","text":"<p>Now that the Ingress controller and certificate manager are setup the Ingress resource can be configured accordingly by updating the <code>values.yaml</code>.</p>"},{"location":"examples/ingress/INGRESS_NGINX/#1-ingress-resource-config","title":"1. Ingress resource config","text":"<p>For TLS cert auto-provisioning and TLS termination update the <code>ingress</code> stanza within the products <code>values.yaml</code>: <pre><code>ingress:\ncreate: true\nnginx: true\nmaxBodySize: 250m\nhost: &lt;dns_record&gt;\npath: \"/\"\nannotations:\ncert-manager.io/issuer: \"letsencrypt-prod\" # Using https://letsencrypt.org/\nhttps: true\ntlsSecretName: tls-certificate\n</code></pre></p> <p>Configuring the <code>host</code> value</p> <p>In this case the <code>&lt;dns_record&gt;</code> would correspond to the record name that was created in 3. DNS setup above</p>"},{"location":"examples/ingress/INGRESS_NGINX/#bitbucket-ssh-configuration","title":"Bitbucket SSH configuration","text":"<p>Additional configuration</p> <p>Bitbucket requires additional Ingress config to allow for <code>SSH</code> access. See NGINX Ingress controller config for SSH connections for details.</p> <p>Next step - Database</p> <p>Having created the Ingress controller continue with provisioning the next piece of prerequisite infrastructure, the database.</p>"},{"location":"examples/logging/efk/EFK/","title":"Logging in a Kubernetes environment","text":"<p>Warning</p> <p>This functionality is not officially supported. This document explains how to enable aggregated logging in your Kubernetes cluster. There are many ways to do this and this document showcases only a few of the options.</p>"},{"location":"examples/logging/efk/EFK/#efk-stack","title":"EFK stack","text":"<p>A common Kubernetes logging pattern is the combination of <code>Elasticsearch</code>, <code>Fluentd</code>, and <code>Kibana</code>, known as EFK Stack. </p> <p>Fluentd is an open-source and multi-platform log processor that collects data/logs from different sources, aggregates, and forwards them to multiple destinations. It is fully compatible with Docker and Kubernetes environments. </p> <p>Elasticsearch is a distributed open search and analytics engine for all types of data. </p> <p>Kibana is an open-source front-end application that sits on top of Elasticsearch, providing search and data visualization capabilities for data indexed in Elasticsearch.</p> <p>There are different methods to deploy an EFK stack. We provide two deployment methods, the first is deploying EFK locally on Kubernetes, and the second is using a managed Elasticsearch instance outside the Kubernetes cluster. </p>"},{"location":"examples/logging/efk/EFK/#local-efk-stack","title":"Local EFK stack","text":"<p>This solution deploys the EFK stack inside the Kubernetes cluster. By setting <code>fluentd.enabled</code> value to <code>true</code>, Helm installs Fluentd on each of application pods. This means that after deployment all the product pods run Fluentd, which collects all the log files and sends them to the Fluentd aggregator container. </p> <p>To complete the EFK stack you need to install an Elasticsearch cluster and Kibana, and successfully forward the aggregated datalog to Elasticsearch using Fluentd, which is already installed. </p> <p>Follow these steps to install Elasticsearch</p>"},{"location":"examples/logging/efk/EFK/#1-install-elasticsearch","title":"1. Install Elasticsearch","text":"<p>Install Elasticsearch using the instructions documented here. Once installed make sure Elasticsearch cluster is working as expected by first port forwarding the service</p> <pre><code>kubectl port-forward svc/elasticsearch-master 9200\n</code></pre> <p>you can then <code>curl</code> the endpoint for the current state</p> <pre><code>$ curl localhost:9200\n{\n\"name\" : \"elasticsearch-master-0\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"cluster_uuid\" : \"uNdYC-2nSdWVdzPCw9P7jQ\",\n  \"version\" : {\n\"number\" : \"7.12.0\",\n       \"build_flavor\" : \"default\",\n       \"build_type\" : \"docker\",\n       \"build_hash\" : \"78722783c38caa25a70982b5b042074cde5d3b3a\",\n       \"build_date\" : \"2021-03-18T06:17:15.410153305Z\",\n       \"build_snapshot\" : false,\n       \"lucene_version\" : \"8.8.0\",\n       \"minimum_wire_compatibility_version\" : \"6.8.0\",\n       \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n},\n  \"tagline\" : \"You Know, for Search\"\n}\n</code></pre>"},{"location":"examples/logging/efk/EFK/#2-enable-fluentd","title":"2. Enable Fluentd","text":"<p>Now enable <code>Fluentd</code> and set the <code>hostname</code> for Elasticsearch in <code>values.yaml</code> as follows:</p> <p><pre><code>fluentd:\nenabled: true\nelasticsearch:\nhostname: elasticsearch-master\n</code></pre> Fluentd tries to parse and send the data to Elasticsearch, but since it's not installed the data is lost. At this point you have logged data in the installed Elasticsearch, and you should install Kibana to complete the EFK stack deployment:</p>"},{"location":"examples/logging/efk/EFK/#3-install-kibana","title":"3. Install Kibana","text":"<p>With the same version that was used for installing Elasticsearch, use the <code>imageTag</code> property to install Kibana:</p> <pre><code>helm install kibana --namespace &lt;namespace&gt; --set imageTag=\"7.9.3\" elastic/kibana\n</code></pre> <p>Make sure kibana is running by checking the deployment</p> <p><pre><code>kubectl get deployment\n</code></pre> You should see something like... <pre><code>NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\nhelm-operator                      1/1           1            1     23m\ningress-nginx-release-controller   1/1           1            1     22m\nkibana-kibana                      1/1           1            1     25m\n</code></pre> Through port-forwarding you can access Kibana via <code>http://localhost:5601</code> <pre><code>kubectl port-forward deployment/kibana-kibana 5601\n</code></pre>  To visualise the logs you need to create an index pattern and then look at the the data in the discovery part. To create the index pattern go to <code>Management</code> \u2192 <code>Stack Management</code> and then select <code>Kibana</code> \u2192 <code>Index Patterns</code>. </p>"},{"location":"examples/logging/efk/EFK/#managed-efk-stack","title":"Managed EFK stack","text":"<p>In this solution Elasticsearch is deployed as a managed AWS service and lives outside of the Kubernetes cluster. This approach uses Fluentbit instead of <code>Fluentd</code> for log processing.</p> Fluentbit <p><code>Fluentbit</code> is used to collect and aggregate log data inside the EKS cluster. It then sends this to an AWS Elasticsearch instance outside of the cluster.</p> <p>When a node inside an EKS cluster needs to call an AWS API, it needs to provide extended permissions. Amazon provides an image of <code>Fluentbit</code> that supports AWS service accounts,and using this you no longer need to follow the traditional way. All you need is to have an IAM role for the AWS service account on an EKS cluster. Using this service account, an AWS permission can be provided to the containers in any pod that use that service account. The result is that the pods on that node can call AWS APIs.</p> <p>Your first step is to configure IAM roles for Service Accounts (IRSA) for <code>Fluentbit</code>, to make sure you have an OIDC identity provider to use IAM roles for the service account in the cluster:</p> <p><pre><code>eksctl utils associate-iam-oidc-provider \\\n--cluster &lt;cluster_name&gt; \\\n--approve </code></pre> Then create an IAM policy to limit the permissions to connect to the Elasticsearch cluster. Before this, you need to set the following environment variables: </p> Environment variable Value KUBE_NAMESPACE The namespace for kubernetes cluster ES_DOMAIN_NAME Elasticsearch domain name ES_VERSION Elasticsearch version ES_USER Elasticsearch username ES_PASSWORD Elasticsearch password (eg. <code>export ES_PASSWORD=\"$(openssl rand -base64 8)_Ek1$\"</code>) ACCOUNT_ID AWS Account ID AWS_REGION AWS region code <p>Now create the file <code>fluent-bit-policy.json</code> to define the policy itself:</p> <p><pre><code>cat &lt;&lt;EoF &gt; ~/environment/logging/fluent-bit-policy.json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n         {\n             \"Action\": [\n                 \"es:ESHttp*\"\n             ],\n             \"Resource\": \"arn:aws:es:${AWS_REGION}:${ACCOUNT_ID}:domain/${ES_DOMAIN_NAME}\",\n             \"Effect\": \"Allow\"\n         }\n    ]\n}\nEoF\n</code></pre> Next initialize the policy: <pre><code>aws iam create-policy  \\\n--policy-name fluent-bit-policy \\\n--policy-document file://~/environment/logging/fluent-bit-policy.json\n</code></pre> Create an IAM role for the service account: <pre><code>eksctl create iamserviceaccount \\\n--name fluent-bit \\\n--namespace dcd \\\n--cluster dcd-ap-southeast-2 \\\n--attach-policy-arn \"arn:aws:iam::${ACCOUNT_ID}:policy/fluent-bit-policy\" \\\n--approve \\\n--override-existing-serviceaccounts\n</code></pre> Confirm that the service account with an Amazon Resource Name (ARN) of the IAM role is annotated: <pre><code>kubectl describe serviceaccount fluent-bit\n</code></pre> Look for output similar to: <pre><code>Name: fluent-bit\nNamespace:  elastic\nLabels: &lt;none&gt;\nAnnotations: eks.amazonaws.com/role-arn: arn:aws:iam::000000000000:role/eksctl-your-cluster-name-addon-iamserviceac-Role1-0A0A0A0A0A0A0\nImage pull secrets: &lt;none&gt;\nMountable secrets:  fluent-bit-token-pgpss\nTokens:  fluent-bit-token-pgpss\nEvents:  &lt;none&gt;\n</code></pre> Now define the Elasticsearch domain</p> <p>This configuration will provision a public Elasticsearch cluster with Fine-Grained Access Control enabled and a built-in user database:</p> <p><pre><code>cat &lt;&lt;EOF&gt; ~/environment/logging/elasticsearch_domain.json\n{\n    \"DomainName\": ${ES_DOMAIN_NAME},\n    \"ElasticsearchVersion\": ${ES_VERSION},\n    \"ElasticsearchClusterConfig\": {\n         \"InstanceType\": \"r5.large.elasticsearch\",\n         \"InstanceCount\": 1,\n             \"DedicatedMasterEnabled\": false,\n             \"ZoneAwarenessEnabled\": false,\n             \"WarmEnabled\": false\n         },\n    \"EBSOptions\": {\n         \"EBSEnabled\": true,\n         \"VolumeType\": \"gp2\",\n         \"VolumeSize\": 100\n    },\n    \"AccessPolicies\": \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{\\\"AWS\\\":\\\"*\\\"},\\\"Action\\\":\\\"es:ESHttp*\\\",\\\"Resource\\\":\\\"arn:aws:es:${AWS_REGION}:${ACCOUNT_ID}:domain/${ES_DOMAIN_NAME}/*\\\"}]}\",\n    \"SnapshotOptions\": {},\n    \"CognitoOptions\": {\n         \"Enabled\": false\n    },\n    \"EncryptionAtRestOptions\": {\n         \"Enabled\": true\n    },\n    \"NodeToNodeEncryptionOptions\": {\n         \"Enabled\": true\n    },\n    \"DomainEndpointOptions\": {\n         \"EnforceHTTPS\": true,\n         \"TLSSecurityPolicy\": \"Policy-Min-TLS-1-0-2019-07\"\n    },\n    \"AdvancedSecurityOptions\": {\n         \"Enabled\": true,\n         \"InternalUserDatabaseEnabled\": true,\n         \"MasterUserOptions\": {\n             \"MasterUserName\": ${ES_USER},\n             \"MasterUserPassword\": ${ES_PASSWORD}\n         }\n    }\n}\nEOF\n</code></pre> Initialize the Elasticsearch domain using the <code>elasticsearch_domain.json</code></p> <pre><code>aws es create-elasticsearch-domain \\\n--cli-input-json   file://~/environment/logging/elasticsearch_domain.json\n</code></pre> <p>It takes a while for Elasticsearch clusters to change to an active state. Check the AWS Console to see the status of the cluster, and continue to the next step when the cluster is ready.</p> <p>At this point you need to map roles to users in order to set fine-grained access control, because without this mapping all the requests to the cluster will result in permission errors. You should add the <code>Fluentbit</code> ARN as a backend role to the <code>all-access</code> role, which uses the Elasticsearch APIs. To find the <code>fluentbit</code> ARN run the following command and export the value of <code>ARN Role</code> into the <code>FLUENTBIT_ROLE</code> environment variable: <pre><code>eksctl get iamserviceaccount --cluster dcd-ap-southeast-2\n</code></pre> The output of this command should look similar to this: <pre><code>NAMESPACE    NAME                ROLE ARN\nkube-system cluster-autoscaler   arn:aws:iam::887464544476:role/eksctl-dcd-ap-southeast-2-addon-iamserviceac-Role1-1RSRFV0BQVE3E\n</code></pre> Take note of the <code>ROLE ARN</code> and export it as the environment variable <code>FLUENTBIT_ROLE</code> <pre><code>export FLUENTBIT_ROLE=arn:aws:iam::887464544476:role/eksctl-dcd-ap-southeast-2-addon-iamserviceac-Role1-1RSRFV0BQVE3E\n</code></pre> Retrieve the Elasticsearch endpoint and update the internal database: <pre><code>export ES_ENDPOINT=$(aws es describe-elasticsearch-domain --domain-name ngh-search-domain --output text --query \"DomainStatus.Endpoint\")\n</code></pre></p> <p><pre><code>curl -sS -u \"${ES_DOMAIN_USER}:${ES_DOMAIN_PASSWORD}\" \\\n-X PATCH \\\nhttps://${ES_ENDPOINT}/_opendistro/_security/api/rolesmapping/all_access?pretty \\\n-H 'Content-Type: application/json' \\\n-d'\n[\n   {\n     \"op\": \"add\", \"path\": \"/backend_roles\", \"value\": [\"'${FLUENTBIT_ROLE}'\"]\n   }\n]\n'\n</code></pre> Finally, it is time to deploy the <code>Fluentbit</code> DaemonSet: <pre><code>kubectl apply -f docs/docs/examples/logging/efk/managed_es/fluentbit.yaml\n</code></pre> After a few minutes all pods should be up and in running status. you can open Kibana to visualise the logs. The endpoint for Kibana can be found in the Elasticsearch output tab in the AWS console, or you can run the following command: <pre><code>echo \"Kibana URL: https://${ES_ENDPOINT}/_plugin/kibana/\" \nKibana URL: https://search-domain-uehlb3kxledxykchwexee.ap-southeast-2.es.amazonaws.com/_plugin/kibana/\n</code></pre></p> <p>The user and password for Kibana are the same as the master user credential that is set in Elasticsearch in the provisioning stage. Open Kibana in a browser and after login, create an index pattern and see the report in the <code>Discover</code> page. </p>"},{"location":"examples/storage/STORAGE/","title":"Shared storage","text":"<p>Atlassian's Data Center products require a shared storage solution to effectively operate in multi-node environment. The specifics of how this shared storage is created is site-dependent, we do however provide examples on how shared storage can be created below.</p> <p>Due to the high requirements on performance for IO operations, Bitbucket needs a dedicated NFS server providing persistence for a shared home. See NFS example for details</p>"},{"location":"examples/storage/STORAGE/#aws-efs","title":"AWS EFS","text":"<p>Jira, Confluence and Crowd can all be configured with an EFS-backed shared solution. For details on how this can be set up, see the AWS EFS example. </p>"},{"location":"examples/storage/STORAGE/#nfs","title":"NFS","text":"<p>For details on creating shared storage for Bitbucket, see the NFS example.</p>"},{"location":"examples/storage/additional%20storage/ADDTIONAL_STORAGE/","title":"Additional Storage","text":"<p>You can use volumeClaimTemplates to have additional storage. This is useful when your enviroment uses several types of storage. </p> <p>E.g. If you want to deploy Confluence on NFS, but you want to use BlockStorage (or everything else instead of NFS) for the lucene-index, you can create extra volumn for BlockStorage by defining <code>volumeClaimTemplates</code> in <code>values.yaml</code> then mount the volume in <code>additionalVolumeMounts</code>. <pre><code>confluence:\nadditionalVolumeClaimTemplates:\n- name: myadditionalvolumeclaim\nstorageClassName: gp2\nresources:\nrequests:\nstorage: 1Gi\nadditionalVolumeMounts:\n- mountPath: /var/atlassian/application-data/confluence/index\nname: myadditionalvolumeclaim\n</code></pre></p>"},{"location":"examples/storage/aws/LOCAL_STORAGE/","title":"Local storage","text":"<p>This file provides examples on how a Kubernetes cluster and helm deployment can be configured to utilize AWS EBS backed volumes.</p>"},{"location":"examples/storage/aws/LOCAL_STORAGE/#dynamic-provisioning","title":"Dynamic provisioning","text":"<p>Due to the ephemeral nature of Kubernetes pods we advise dynamic provisioning be used for creating and consuming EBS volume(s).</p>"},{"location":"examples/storage/aws/LOCAL_STORAGE/#prerequisites","title":"Prerequisites","text":"<p>Ensure the EBS CSI driver is installed within the k8s cluster, you can confirm this by running:  </p> <p><pre><code>kubectl get csidriver\n</code></pre> the output of the above command should include the named driver <code>ebs.csi.aws.com</code> for example: <pre><code>NAME              ATTACHREQUIRED   PODINFOONMOUNT   MODES        AGE\nebs.csi.aws.com   true             false            Persistent   5d1h\n</code></pre> If not present the EBS driver can be installed using the following instructions here.</p>"},{"location":"examples/storage/aws/LOCAL_STORAGE/#provisioning","title":"Provisioning","text":"<ol> <li>Create a Storage Class</li> <li>Update <code>values.yaml</code> to utilise Storage Class</li> </ol>"},{"location":"examples/storage/aws/LOCAL_STORAGE/#1-create-storage-class","title":"1. Create Storage Class","text":"<pre><code>kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\nname: ebs-sc\nprovisioner: ebs.csi.aws.com\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre>"},{"location":"examples/storage/aws/LOCAL_STORAGE/#2-update-valuesyaml","title":"2. Update values.yaml","text":"<p>Update the <code>localHome</code> <code>storageClassName</code> value within <code>values.yaml</code> to the name of the Storage Class created in step 1 above</p> <pre><code>volumes:\nlocalHome:\npersistentVolumeClaim:\ncreate: true\nstorageClassName: \"ebs-sc\"\n</code></pre>"},{"location":"examples/storage/aws/LOCAL_STORAGE/#resources","title":"Resources","text":"<p>Some useful resources on provisioning local storage with the AWS CSI Driver</p> <ul> <li>EBS CSI driver - GitHub Repo</li> <li>Official Amazon EBS CSI driver documentation</li> </ul> <p>Product installation</p> <p>Creating the local home volume is the final step in provisioning the required infrastructure. You can now move onto the next step, Installation.</p>"},{"location":"examples/storage/aws/SHARED_STORAGE/","title":"Shared storage","text":"<p>This file provides examples on how a Kubernetes cluster and helm deployment can be configured to utilize an AWS EFS backed filesystem.</p>"},{"location":"examples/storage/aws/SHARED_STORAGE/#static-provisioning","title":"Static provisioning","text":"<p>An example detailing how an existing EFS filesystem can be created and consumed using static provisioning.</p>"},{"location":"examples/storage/aws/SHARED_STORAGE/#prerequisites","title":"Prerequisites","text":"<ol> <li>EFS CSI driver is installed within the k8s cluster.</li> <li>A physical EFS filesystem has been provisioned</li> </ol> <p>Additional details on static EFS provisioning can be found here</p> <p>You can confirm that the EFS CSI driver has been installed by running:</p> <p><pre><code>kubectl get csidriver\n</code></pre> the output of the above command should include the named driver <code>efs.csi.aws.com</code> for example: <pre><code>NAME              ATTACHREQUIRED   PODINFOONMOUNT   MODES        AGE\nefs.csi.aws.com   false            false            Persistent   5d3h\n</code></pre></p>"},{"location":"examples/storage/aws/SHARED_STORAGE/#provisioning","title":"Provisioning","text":"<ol> <li>Create a Persistent Volume</li> <li>Create a Persistent Volume Claim</li> <li>Update <code>values.yaml</code> to utilise Persistent Volume Claim</li> </ol>"},{"location":"examples/storage/aws/SHARED_STORAGE/#1-create-persistent-volume","title":"1. Create Persistent Volume","text":"<p>Create a persistent volume for the pre-provisioned EFS filesystem by providing the <code>&lt;efs-id&gt;</code>. The EFS id can be identified using the CLI command below with the appropriate region</p> <pre><code>aws efs describe-file-systems --query \"FileSystems[*].FileSystemId\" --region ap-southeast-2\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: my-shared-vol-pv\nspec:\ncapacity:\nstorage: 1Gi\nvolumeMode: Filesystem\naccessModes:\n- ReadWriteMany\nstorageClassName: efs-pv\npersistentVolumeReclaimPolicy: Retain\nmountOptions:\n- rw\n- lookupcache=pos\n- noatime\n- intr\n- _netdev\ncsi:\ndriver: efs.csi.aws.com\nvolumeHandle: &lt;efs-id&gt;\n</code></pre>"},{"location":"examples/storage/aws/SHARED_STORAGE/#2-create-persistent-volume-claim","title":"2. Create Persistent Volume Claim","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: my-shared-vol-pvc\nspec:\naccessModes:\n- ReadWriteMany\nstorageClassName: efs-pv\nvolumeMode: Filesystem\nvolumeName: my-shared-vol-pv\nresources:\nrequests:\nstorage: 1Gi\n</code></pre>"},{"location":"examples/storage/aws/SHARED_STORAGE/#3-update-valuesyaml","title":"3. Update values.yaml","text":"<p>Update the <code>sharedHome</code> <code>claimName</code> value within <code>values.yaml</code> to the name of the Persistent Volume Claim created in step 2 above</p> <pre><code>volumes:\nsharedHome:\ncustomVolume:\npersistentVolumeClaim:\nclaimName: \"my-shared-vol-pvc\"\n</code></pre>"},{"location":"examples/storage/aws/SHARED_STORAGE/#resources","title":"Resources","text":"<p>Some useful resources on provisioning shared storage with the AWS CSI Driver:</p> <ul> <li>Amazon EFS CSI driver</li> <li>Introducing Amazon EFS CSI dynamic provisioning</li> </ul> <p>Next step - Local storage</p> <p>Having created the shared home volume continue with provisioning the next piece of prerequisite infrastructure, local storage.</p>"},{"location":"examples/storage/aws/s3/CONFLUENCE/","title":"AWS S3 Attachments Storage","text":"<p>Since 8.1.0 Confluence supports storing attachments in AWS S3. To enable this feature, update the image <code>tag</code> to <code>8.1.0</code> and define bucket name and AWS region in <code>confluence.s3AttachmentsStorage</code>, for example:</p> <pre><code>tag: 8.1.0\nconfluence:\ns3AttachmentsStorage:\nbucketName: confluence-attachments-bucket\nbucketRegion: us-east-1\n</code></pre>"},{"location":"examples/storage/aws/s3/CONFLUENCE/#aws-authentication","title":"AWS Authentication","text":"<p>You will find details on available authentication methods in Credential Provider.</p> <p>Make sure <code>ATL_UNSET_SENSITIVE_ENV_VARS</code> is set to false if you choose to define <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> in <code>confluence.additionalEnvironmentVariables</code>:</p> <pre><code>additionalEnvironmentVariables:\n- name: AWS_ACCESS_KEY_ID\nvalueFrom:\nsecretKeyRef:\nname: aws-creds\nkey: AWS_ACCESS_KEY_ID\n- name: AWS_SECRET_ACCESS_KEY\nvalueFrom:\nsecretKeyRef:\nname: aws-creds\nkey: AWS_SECRET_ACCESS_KEY\n- name: ATL_UNSET_SENSITIVE_ENV_VARS\nvalue: \"false\"\n</code></pre>"},{"location":"examples/storage/aws/s3/CONFLUENCE/#eks-irsa","title":"EKS IRSA","text":"<p>If Confluence is deployed to AWS EKS, it is strongly recommended to use IAM roles for service accounts (IRSA).</p> <p>The Confluence service account will be automatically annotated with a role <code>ARN</code> if it is defined, for example:</p> <pre><code>serviceAccount:\neksIrsa:\nroleArn: arn:aws:iam::37583956:role/confluence-s3-role\n</code></pre> <p>Below is an example policy, providing appropriate S3 access to Confluence, that needs to be attached to the role:</p> <pre><code>{\n\"Statement\": [\n{\n\"Action\": [\n\"s3:PutObject\",\n\"s3:ListBucket\",\n\"s3:GetObject\",\n\"s3:DeleteObject\"\n],\n\"Effect\": \"Allow\",\n\"Resource\": [\n\"arn:aws:s3:::confluence-attachments-bucket/*\",\n\"arn:aws:s3:::confluence-attachments-bucket\"\n],\n\"Sid\": \"\"\n}\n],\n\"Version\": \"2012-10-17\"\n}\n</code></pre>"},{"location":"examples/storage/nfs/NFS/","title":"NFS server for Bitbucket","text":"<p>Disclaimer</p> <p>This functionality is not officially supported. It should not be used for production deployments!</p> <p>The included NFS example is provided as is and should be used as reference a only. Before you proceed we highly recommend that you understand your specific deployment needs and tailor your solution to them.</p>"},{"location":"examples/storage/nfs/NFS/#bitbucket-data-center-and-nfs","title":"Bitbucket Data Center and NFS","text":"<p>Due to the high performance requirements on IO operations, Bitbucket needs a dedicated NFS server providing persistence for a shared home. For this reason  we don't recommend that you use cloud managed storage services such as AWS EFS.</p>"},{"location":"examples/storage/nfs/NFS/#nfs-provisioning","title":"NFS provisioning","text":"<p>The NFS server can be provisioned manually or by using the supplied Helm chart. Details for both approaches can be found below.</p> <p>Pod affinity</p> <p>To reduce the IO latency between the NFS server and Bitbucket Pod(s) it is  highly recommend to keep them in close proximity. To achieve this, you can use standard Kubernetes affinity rules. The <code>affinity</code> stanza within <code>values.yaml</code> can be updated to take advantage of this behaviour i.e.</p>"},{"location":"examples/storage/nfs/NFS/#manual","title":"Manual","text":"<p>For information on setting up Bitbucket Data Center's shared file server, see Provision your shared file system.  This section contains the requirements and recommendations for setting up NFS for Bitbucket Data Center.</p> <p>NFS Server sizing</p> <p>Ensure the NFS server's size is appropriate for the needs of the Bitbucket instance. See capacity recommendations for details.</p>"},{"location":"examples/storage/nfs/NFS/#helm","title":"Helm","text":"<p>Disclaimer</p> <p>This Helm chart is not officially supported! It should not be used for production deployments!</p>"},{"location":"examples/storage/nfs/NFS/#installation","title":"Installation","text":"<p>Create a namespace for the NFS <pre><code>kubectl create namespace nfs\n</code></pre> Clone this repository and from the sub-directory, <code>data-center-helm-charts/docs/docs/examples/storage/nfs</code>, run the following command: <pre><code>helm install nfs-server nfs-server-example --namespace nfs\n</code></pre></p>"},{"location":"examples/storage/nfs/NFS/#uninstall","title":"Uninstall","text":"<pre><code>helm uninstall nfs-server --namespace nfs\n</code></pre>"},{"location":"examples/storage/nfs/NFS/#update-valuesyaml","title":"Update <code>values.yaml</code>","text":"<p>Get the IP address of the NFS service (<code>CLUSTER-IP</code>) by running the following command <pre><code>kubectl get service --namespace nfs -o jsonpath='{.items[0].spec.clusterIP}'\n</code></pre></p> <p>NFS directory share</p> <p>The NFS Helm chart creates and exposes the directory share <code>/srv/nfs</code>. This will be required when configuring <code>values.yaml</code> </p> <p>The approach below shows how a <code>persistentVolume</code> and corresponding <code>peristentVolumeClaim</code> can be dynamically created for the provisioned NFS. Using the NFS IP and directory share, (see above) update the <code>values.yaml</code> appropriately: <pre><code>volumes:\nsharedHome:\npersistentVolume:\ncreate: true\nnfs:\nserver: \"10.100.197.23\" # IP address of the NFS server \npath: \"/srv/nfs\" # Directory share of NFS\npersistentVolumeClaim:\ncreate: true\nstorageClassName: \"\"\n</code></pre> You can of course manually provision your own <code>persistentVolume</code> and corresponding claim (as opposed to the dynamic approach described above) for the NFS server. In this case update the <code>values.yaml</code> to make use of them via the <code>customVolume</code> stanza, <code>sharedHome.persistentVolume.create</code> and <code>sharedHome.persistentVolumeClaim.create</code> should also both be set to <code>false</code>. <pre><code>sharedHome:\npersistentVolume:\ncreate: false\npersistentVolumeClaim:\ncreate: false\ncustomVolume: persistentVolumeClaim:\nclaimName: \"custom-nfs-server-claim\"\n</code></pre></p> <p>Next step - Local storage</p> <p>Having created the shared home NFS continue with provisioning the next piece of prerequisite infrastructure, local storage.</p>"},{"location":"platforms/OPENSHIFT/","title":"OpenShift","text":"<p>Support Disclaimer</p> <p>Helm is a Kubernetes package manager that orchestrates the provisioning of applications onto existing Kubernetes infrastructure. The requirements for this infrastructure are described in Prerequisites. The Kubernetes cluster remains your responsibility; we do not provide direct support for Kubernetes or the underlying hardware it runs on.</p> <p>If you have followed our documentation on how to configure the Helm charts, and you're using correctly created components, we will then provide support if you encounter an error with installation after running the <code>helm install</code> command.</p> <p>Read more about what we support and what we don\u2019t.</p> <p>The Helm charts are vendor agnostic and create objects from standard APIs that OpenShift fully supports.</p> <p>However, by default OpenShift will not allow running containers as users specified in the image <code>Dockerfiles</code> or <code>securityContext.fsGroup</code> in a statefulset/deployment spec. You will see the following error in stateful set events if you deploy with default Helm chart values:</p> <pre><code>create Pod jira-0 in StatefulSet jira failed error: pods \"jira-0\" is forbidden: unable to validate against any security context constraint: [provider \"anyuid\": Forbidden: not usable by user or serviceaccount, provider restricted: .spec.securityContext.fsGroup: Invalid value: []int64{2001}: 2001 is not an allowed group, spec.initContainers[0].securityContext.runAsUser: Invalid value: 0: must be in the ranges: [1003170000, 1003179999]\n</code></pre> <p>There are a couple of ways to fix this.</p>"},{"location":"platforms/OPENSHIFT/#attach-anyuid-policies","title":"Attach <code>anyuid</code> policies","text":"<p>If possible, attach <code>anyuid</code> policy to 2 serviceAccounts. Here's an example for a Bitbucket installation. Please, note that the service account names vary depending on the Data Center product:</p> For Bitbucket podsFor NFS permission fixer pod <pre><code>oc adm policy add-scc-to-user anyuid -z bitbucket -n git\n</code></pre> <pre><code>oc adm policy add-scc-to-user anyuid -z default -n git\n</code></pre> <p>Typically, the <code>volumes.sharedHome.persistentVolumeClaim.nfsPermissionFixer</code> needs to be set to <code>true</code> to make volume writable. It depends on the storage backend though.</p>"},{"location":"platforms/OPENSHIFT/#disable-security-context","title":"Disable security context","text":"<p>As an alternative, (if letting containers run as pre-defined users is not possible), set <code>product_name.securityContextEnabled</code> to <code>false</code>, for example, <code>confluence.securityContextEnabled: false</code>. As a result the container will start as a user with an OpenShift generated ID. You will also need to disable NFS permission fixer init container as it starts as root. Set <code>volumes.sharedHome.nfsPermissionFixer.enabled</code> to false.</p>"},{"location":"platforms/OPENSHIFT/#permission-issues","title":"Permission issues","text":"<p>If a container starts without <code>anyuid</code> enabled, applications can't write to <code>${APPLICATION_HOME}/logs</code>, <code>${APPLICATION_HOME}/work</code> and <code>${APPLICATION_HOME}/temp</code>. If you see in logs that the server fails to start with <code>permission denied</code> errors, you may want to declare these directories as runtime volumes. To do so, you need to declare additional volume mounts and additional volumes in <code>values.yaml</code>:</p> <pre><code>confluence:\n  additionalVolumeMounts:\n    - name: tomcat-work\n      # this example is for Confluence\n      mountPath: /opt/atlassian/confluence/work\nvolumes:\n  additional:\n    - name: tomcat-work\n      emptyDir: {}\n</code></pre> <p>While it's possible to declare runtime volumes for empty directories, it is not possible for <code>${APPLICATION_HOME}/conf</code>. When starting up, Jira and Confluence generate a few configuration files which is a part of the image entrypoint. Without <code>anyuid</code> SCC, an unprivileged user can't write to <code>${APPLICATION_HOME}/conf</code>. When starting Jira in OpenShift without <code>anyuid</code> SCC attached to jira service account, you will see the following log:</p> <pre><code>INFO:root:Generating /etc/container_id from template container_id.j2\nWARNING:root:Permission problem writing '/etc/container_id'; skipping\nINFO:root:Generating /opt/atlassian/jira/conf/server.xml from template server.xml.j2\nWARNING:root:Permission problem writing '/opt/atlassian/jira/conf/server.xml'; skipping\nINFO:root:Generating /opt/atlassian/jira/atlassian-jira/WEB-INF/classes/seraph-config.xml from template seraph-config.xml.j2\nWARNING:root:Permission problem writing '/opt/atlassian/jira/atlassian-jira/WEB-INF/classes/seraph-config.xml'; skipping\nINFO:root:Generating /var/atlassian/application-data/jira/dbconfig.xml from template dbconfig.xml.j2\nWARNING:root:Could not chown path /var/atlassian/application-data/jira/dbconfig.xml to jira:jira due to insufficient permissions.\nINFO:root:Running Jira with command '/opt/atlassian/jira/bin/start-jira.sh', arguments ['/opt/atlassian/jira/bin/start-jira.sh', '-fg']\nexecuting as current user\n</code></pre> <p>While this is a non-fatal error and Jira is able to proceed with startup, failure to properly generate configuration files like server.xml will result in a number of errors when using Jira.</p> <p>To mitigate the problem, either attach anyuid policy to Jira (or Confluence) service account or build your own container image if existing security practices do not allow anyuid. You need to inherit the Jira (or Confluence) official image and make a few directories/files writable for users belonging to a <code>root</code> group (which users in OpenShfit containers belong to):</p> <pre><code>FROM atlassian/jira-software:$JIRA_VERSION\nRUN chgrp -R 0 /opt/atlassian/jira/conf &amp;&amp; chmod -R g=u /opt/atlassian/jira/conf &amp;&amp; \\\n    chgrp 0 /etc/container_id &amp;&amp; chmod g=u /etc/container_id\n</code></pre>"},{"location":"platforms/OPENSHIFT/#openshift-routes","title":"OpenShift Routes","text":"<p>The Helm charts do not have templates for OpenShift routes that are commonly used in OpenShift instead of ingresses. Routes need to be manually created after the charts installation.</p>"},{"location":"platforms/PLATFORMS/","title":"Platform information","text":"<p>Support Disclaimer</p> <p>Helm is a Kubernetes package manager that orchestrates the provisioning of applications onto existing Kubernetes infrastructure. The requirements for this infrastructure are described in Prerequisites. The Kubernetes cluster remains your responsibility; we do not provide direct support for Kubernetes or the underlying hardware it runs on.</p> <p>If you have followed our documentation on how to configure the Helm charts, and you're using correctly created components, we will then provide support if you encounter an error with installation after running the <code>helm install</code> command. </p> <p>Read more about what we support and what we don\u2019t. </p> <p>Using our Helm charts on different platforms:</p> <ul> <li>OpenShift</li> </ul>"},{"location":"troubleshooting/LIMITATIONS/","title":"Limitations","text":""},{"location":"troubleshooting/LIMITATIONS/#product-limitations","title":"Product limitations","text":"<p>We haven't changed our Data Center applications' architecture to support Kubernetes. So, as is with all our Data Center products, the following limitations still exist:</p> <ul> <li>We don't support horizontal or vertical autoscaling in our products. Read about Product scaling.</li> <li>More pods doesn't mean that the application will be more performant.</li> <li>We still have session affinity, so you will need to have a network setup that supports that. </li> </ul>"},{"location":"troubleshooting/LIMITATIONS/#jira-and-horizontal-scaling","title":"Jira and horizontal scaling","text":"<p>At present there are issues relating to index replication with Jira when immediately scaling up by more than 1 pod at a time.</p> <ul> <li>Index replication service is paused indefinitely</li> <li>Automatic restore of indexes will fail </li> </ul> <p>Indexing improvements</p> <p>Please note that Jira is actively being worked on to address these issues in the coming releases.</p> <p>Although these issues are Jira specific, they are exasperated on account of the significantly reduced startup times for Jira when running in a Kubernetes cluster. As such these issues can have an impact on horizontal scaling if you don't take the correct approach.</p>"},{"location":"troubleshooting/LIMITATIONS/#bamboo","title":"Bamboo","text":"<p>There are a number of known limitations relating to Bamboo Data Center, these are documented below.</p>"},{"location":"troubleshooting/LIMITATIONS/#deployment","title":"Deployment","text":"<p>With Bamboo DC 8.1 deployments to K8s using the Helm charts are now possible. This release does however contain an issue where partial unattended deployments to K8s do not work. </p> <p>Unattended setup</p> <p>Until this issue has been resolved, the recommended approach for deploying Bamboo server is using an <code>unattended</code> approach. That is, providing values to all those properties labeled as <code>REQUIRED</code> and <code>UNATTENDED-SETUP</code> within the <code>values.yaml</code>. This has the added benefit of eliminating any manual intervention (via the setup wizard) required for configuring Bamboo post deployment.</p> <p>It should also be noted that the property, <code>bamboo.unattendedSetup</code> should be set to <code>true</code> (current default value) for this to work.</p>"},{"location":"troubleshooting/LIMITATIONS/#cluster-size","title":"Cluster size","text":"<p>At present Bamboo Data Center utilizes an active-passive clustering model. This architecture is not ideal where K8s deployments are concerned.</p> <p>1 pod clusters only</p> <p>At present, Bamboo server cluster sizes comprising only <code>1</code> pod is the only supported topology for now.</p>"},{"location":"troubleshooting/LIMITATIONS/#server-and-agent-affinity","title":"Server and agent affinity","text":"<p>The Bamboo server and Bamboo agents must be deployed to the same cluster. You cannot have Bamboo agents in one cluster communicating with a Bamboo server in another.</p>"},{"location":"troubleshooting/LIMITATIONS/#bamboo-to-cloud-app-link","title":"Bamboo to Cloud App Link","text":"<p>When configuring application links between Bamboo server and any Atlassian Cloud server product, the Bamboo server base URL needs to be used. See public issue for more detail.</p>"},{"location":"troubleshooting/LIMITATIONS/#import-and-export-of-large-datasets","title":"Import and export of large datasets","text":"<p>At present there is an issue with Bamboo where the <code>/server</code> and <code>/status</code> endpoints become un-usable when performing an export or import of large datasets. </p> <p>Data migration</p> <p>For large Bamboo instances we recommend using native database and filesystem backup tools instead of the built in export / import functionality that Bamboo provides. See the migration guide for more details.</p> <p>The Bamboo Helm chart does however provide a facility that can be used to import data exports produced through Bamboo at deployment time. This can be used by configuring the Bamboo server <code>values.yaml</code> appropriately i.e. </p> <pre><code>import:\ntype: import\npath: \"/var/atlassian/application-data/shared-home/bamboo-export.zip\"\n</code></pre> <p>Using this approach will restore the full dataset as part of the Helm install process.</p>"},{"location":"troubleshooting/LIMITATIONS/#platform-limitations","title":"Platform limitations","text":"<p>These configurations are explicitly not supported, and the Helm charts don\u2019t work without modifications in these environments:</p> <ul> <li>Istio infrastructure<ul> <li>Due to several reasons, Istio is imposing networking rules on every workload in the Kubernetes cluster that doesn't work with our deployments.</li> <li>The current recommendation is to create an exemption for our workloads if Istio is enabled in the cluster by default.</li> </ul> </li> </ul>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/","title":"Support boundaries","text":"<p>This page describes what is within our scope of support for Kubernetes deployments, and what isn't. </p> Additional information <ul> <li> <p>Read our troubleshooting tips.</p> </li> <li> <p>Read about the product and platform limitations.</p> </li> </ul>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#supported-components","title":"Supported components","text":""},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#helm-charts","title":"Helm charts","text":"<p>Helm is a Kubernetes package manager. It allows us to provide generic <code>YAML</code> templates that you configure with the specific values for your environments.</p> <p>As described in the Prerequisites, you are responsible for creating the components that are required by the product for your type of deployment. You need to supply the appropriate values to your specific <code>values.yaml</code> file that is used for installation. We provide documentation for different configuration options in the Configuration guide and the Examples.</p> <p>If you have followed our documentation on how to configure the Helm charts, and you're using correctly created components, we will then provide support if you encounter an error with installation post <code>helm install</code>. </p> <p>If you find any issues, raise a ticket with our support team. If you have general feedback or questions regarding the charts, use Atlassian Community Kubernetes space.</p>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#unsupported-components","title":"Unsupported components","text":"<p>The Prerequisites can be created in multiple ways. You are responsible for creating them correctly so that they can be used successfully with the Helm charts. Additional details on these prerequisites and their requirements below: </p>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#kubernetes-cluster","title":"Kubernetes cluster","text":"<p>You need to make sure that you have enough privileges to run the application and create all the necessary entities that the Helm charts require. There are also different Kubernetes flavours that might require specific knowledge of how to install the products in them. For example, OpenShift and Rancher have more strict rules regarding container permissions.</p> <p>See examples of provisioning Kubernetes clusters on cloud-based providers.</p>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#shared-storage","title":"Shared storage","text":"<p>Kubernetes setup requires you to have shared storage if you want to have a clustered instance. It's completely up to you how you set up the shared storage. The main requirement is that this storage needs to be accessible from Kubernetes and needs to be accessible from multiple pods. </p> <p>You can use a managed storage solution like EFS, Azure files, or some other dedicated solution that provides NFS-like access (e.g. dedicated NFS server, NetApp).</p> <p>There is a large number of combinations and potential setup scenarios and we can't support all of them. Our Helm charts expect you to provide a persistent volume claim, or a similar accessible shared storage in the <code>values.yaml</code> file.</p> <p>See examples of creating shared storage. For more information about volumes go to the Volumes section of the configuration guide. </p>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#networking","title":"Networking","text":"<p>You're required to configure the network access to the cluster. In Kubernetes, this usually means providing an ingress controller. </p> <p>It is up to you to make sure that the network configuration doesn\u2019t prevent nodes from communicating with each other and other components.</p> <p>You also need to make sure that your instance is accessible to the users (DNS, firewalls, VPC config).</p> <p>See an example of provisioning an NGINX Ingress controller. </p>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#database","title":"Database","text":"<p>Through the <code>values.yaml</code> the database is provided as a connection string with the option to provide credentials and a driver. The database needs to be configured following product-specific requirements and needs to be accessible from Kubernetes.</p> <p>See an example of provisioning databases on cloud-based providers.</p>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#bamboo-custom-remote-agents","title":"Bamboo - custom remote agents","text":"<p>If creating and using custom remote docker agents, you're required to ensure that they are configured, built and working as expected. </p> <p>See an example of customizing a remote agent with bespoke capabilities.</p>"},{"location":"troubleshooting/TROUBLESHOOTING/","title":"Troubleshooting tips","text":"<p>This guide contains general tips on how to investigate an application deployment that doesn't work correctly.</p>"},{"location":"troubleshooting/TROUBLESHOOTING/#general-tips","title":"General tips","text":"<p>First, it is important to gather the information that will help you better understand where to focus your investigation efforts. The next section assumes you've followed the installation and configuration guides, and you can't access the installed product service. </p> <p>For installation troubleshooting, you will need to access the Kubernetes cluster and have enough permissions to follow the commands below.</p> <p>We highly recommend that you read through the Kubernetes official documentation describing monitoring, logging and debugging. Additionally, for great starting tips read the Application Introspection and Debugging section.</p> Value placeholders <p>Some commands include <code>&lt;release_name&gt;</code> and <code>&lt;namespace&gt;</code>. Replace them with the Helm release name and namespace specified when running <code>helm install</code>.</p>"},{"location":"troubleshooting/TROUBLESHOOTING/#my-service-is-not-accessible","title":"My service is not accessible","text":"<p>After <code>helm install</code> finishes, it prints a product service URL. It usually takes a few minutes for the service to start. If you visit the URL too soon, it might return a <code>5XX</code> error HTTP code (the actual code might be dependent on your network implementation).</p> <p>If you have waited long enough (more than 10 minutes), and the service is still not accessible, it is time to investigate the reason why this is the case.</p>"},{"location":"troubleshooting/TROUBLESHOOTING/#helm-release-verification","title":"Helm release verification","text":"<ol> <li>Run <code>helm list --all-namespaces</code> to get the list of all installed chart releases.<ul> <li>You should be able to see your installation in the list</li> <li>The status for the release should be <code>deployed</code></li> </ul> </li> <li>Run <code>helm test &lt;release_name&gt; -n &lt;namespace&gt;</code><ul> <li>This should return application tests in <code>succeeded</code> phase</li> <li>In case there are any test failures you will need to further investigate the particular domain</li> </ul> </li> </ol>"},{"location":"troubleshooting/TROUBLESHOOTING/#dns-verification","title":"DNS verification","text":"<p>To verify the DNS record from your machine, run a basic <code>dig</code> test:</p> <pre><code>dig SERVICE_DOMAIN_NAME\n</code></pre> <p>Or use a web version of the tool.</p>"},{"location":"troubleshooting/TROUBLESHOOTING/#investigate-application-logs","title":"Investigate application logs","text":"<p>You can get application logs from the pods with a standard <code>kubectl</code> command:</p> <pre><code>kubectl logs APPLICATION_POD_NAME\n</code></pre> <p>You can read the output and make sure it doesn't contain an error or an exception.</p>"},{"location":"troubleshooting/TROUBLESHOOTING/#get-application-pod-details","title":"Get application pod details","text":"<p>For more details follow the official guide for debugging.</p> <p>Get the list of pods and their states:</p> <pre><code>kubectl get &lt;release_name&gt; -n &lt;namespace&gt; -o wide\n</code></pre> <p>Get details about a specific pod:</p> <pre><code>kubectl describe POD_NAME -n &lt;namespace&gt;\n</code></pre>"},{"location":"troubleshooting/TROUBLESHOOTING/#get-storage-details","title":"Get storage details","text":"<p>Each application pod needs to have successfully mounted local and shared home. You can find out the details for the persistent volume claims with this command: </p> Prerequisities <p>The example needs to have <code>jq</code> tool installed.</p> <p><pre><code>kubectl get pods --all-namespaces -o=json | jq -c \\\n'.items[] | {name: .metadata.name, namespace: .metadata.namespace, claimName:.spec.volumes[] | select( has (\"persistentVolumeClaim\") ).persistentVolumeClaim.claimName }'\n</code></pre> Find all the application pods in the output and verify they have the correct claims (shared home and local home). For more details follow the documentation for persistent volumes.</p>"},{"location":"userguide/CONFIGURATION/","title":"Configuration","text":""},{"location":"userguide/CONFIGURATION/#ingress","title":"Ingress","text":"<p>In order to make the Atlassian product available from outside of the Kubernetes cluster, a suitable HTTP/HTTPS ingress controller needs to be installed. The standard Kubernetes Ingress resource is not flexible enough for our needs, so a third-party ingress controller and resource definition must be provided. The exact details of the Ingress will be highly site-specific. These Helm charts were tested using the NGINX Ingress Controller. We also provide example instructions on how this controller can be installed and configured.</p> <p>The charts themselves provide a template for Ingress resource rules to be utilised by the provisioned controller. These include all required annotations and optional TLS configuration for the NGINX Ingress Controller.</p> <p>Some key considerations to note when configuring the controller are:</p> <p>Ingress</p> <ul> <li>At a minimum, the ingress needs the ability to support long request timeouts, as well as session affinity (aka \"sticky sessions\").</li> <li> <p>The Ingress Resource provided as part of the Helm charts is geared toward the NGINX Ingress Controller and can be configured via the <code>ingress</code> stanza in the appropriate <code>values.yaml</code>. Some key aspects that can be configured include:</p> <ul> <li>Usage of the NGINX Ingress Controller</li> <li>Ingress Controller annotations</li> <li>The request max body size</li> <li>The hostname of the ingress resource</li> </ul> </li> <li> <p>When installed, with the provided configuration, the NGINX Ingress Controller will provision an internet-facing (see diagram below) load balancer on your behalf. The load balancer should either support the Proxy Protocol or allow for the forwarding of <code>X-Forwarded-*</code> headers. This ensures any backend redirects are done so over the correct protocol.</p> </li> <li>If the <code>X-Forwarded-*</code> headers are being used, then enable the use-forwarded-headers option on the controllers <code>ConfigMap</code>. This ensures that these headers are appropriately passed on.</li> <li>The diagram below provides a high-level overview of how external requests are routed via an internet-facing load balancer to the correct service via Ingress.</li> </ul> <p></p> <p>Traffic flow (diagram)</p> <ol> <li>Inbound client request</li> <li>DNS routes request to appropriate LB</li> <li>LB forwards request to internal Ingress</li> <li>Ingress controller performs traffic routing lookup via Ingress object(s)</li> <li>Ingress forwards request to appropriate service based on Ingress object routing rule</li> <li>Service forwards request to appropriate pod</li> <li>Pod handles request</li> </ol> <p>Request body size</p> <p>By default the maximum allowed size for the request body is set to <code>250MB</code>. If the size in a request exceeds the maximum size of the client request body, an <code>413</code> error will be returned to the client. The maximum request body can be configured by changing the value of <code>maxBodySize</code> in <code>values.yaml</code>.</p>"},{"location":"userguide/CONFIGURATION/#volumes","title":"Volumes","text":"<p>The Data Center products make use of filesystem storage. Each DC node has its own <code>local-home</code> volume, and all nodes in the DC cluster share a single <code>shared-home</code> volume.</p> <p>By default, the Helm charts will configure all of these volumes as ephemeral emptyDir volumes. This makes it possible to install the charts without configuring any volume management, but comes with two big caveats:</p> <ol> <li>Any data stored in the <code>local-home</code> or <code>shared-home</code> will be lost every time a pod starts.</li> <li>Whilst the data that is stored in <code>local-home</code> can generally be regenerated (e.g. from the database), this can be a very expensive process that sometimes requires manual intervention.</li> </ol> <p>For these reasons, the default volume configuration of the Helm charts is suitable only for running a single DC pod for evaluation purposes. Proper volume management needs to be configured in order for the data to survive restarts, and for multi-pod DC clusters to operate correctly.</p> <p>While you are free to configure your Kubernetes volume management in any way you wish, within the constraints imposed by the products, the recommended setup is to use Kubernetes PersistentVolumes and <code>PersistentVolumeClaims</code>.</p> <p>The <code>local-home</code> volume requires a <code>PersistentVolume</code> with ReadWriteOnce (RWO) capability, and <code>shared-home</code> requires a <code>PersistentVolume</code> with ReadWriteMany (RWX) capability. Typically, this will be an NFS volume provided as part of your infrastructure, but some public-cloud Kubernetes engines provide their own <code>RWX</code> volumes (e.g. AWS EFS and Azure Files). While this entails a higher upfront setup effort, it gives the best flexibility.</p>"},{"location":"userguide/CONFIGURATION/#volumes-configuration","title":"Volumes configuration","text":"<p>By default, the charts will configure the <code>local-home</code> and <code>shared-home</code> values as follows:</p> <pre><code>volumes:\n- name: local-home\nemptyDir: {}\n- name: shared-home\nemptyDir: {}\n</code></pre> <p>As explained above, this default configuration is suitable only for evaluation or testing purposes. Proper volume management needs to be configured.</p> <p>Bitbucket default <code>shared-home</code> location</p> <p>For a single node Bitbucket deployment, if no <code>shared-home</code> volume is defined, then a subpath of <code>local-home</code> will automatically be used for this purpose, namely: <code>&lt;LOCAL_HOME_DIRECTORY&gt;/shared</code>. This behaviour is specific to Bitbucket itself and is not orchestrated via the Helm chart.</p> <p>In order to enable the persistence of data stored in these volumes, it is necessary to replace these volumes with something else.</p> <p>The recommended way is to enable the use of <code>PersistentVolume</code> and <code>PersistentVolumeClaim</code> for both volumes, using your install-specific <code>values.yaml</code> file, for example:</p> <pre><code>volumes:\nlocalHome:\npersistentVolumeClaim:\ncreate: true\nsharedHome:\npersistentVolumeClaim:\ncreate: true\n</code></pre> <p>This will result in each pod in the <code>StatefulSet</code> creating a <code>local-home</code> <code>PersistentVolumeClaim</code> of type <code>ReadWriteOnce</code>, and a single <code>PersistentVolumeClaim</code> of type <code>ReadWriteMany</code> being created for the <code>shared-home</code>.</p> <p>For each <code>PersistentVolumeClaim</code> created by the chart, a suitable <code>PersistentVolume</code> needs to be made available prior to installation. These can be provisioned either statically or dynamically, using an auto-provisioner.</p> <p>An alternative to <code>PersistentVolumeClaims</code> is to use inline volume definitions, either for <code>local-home</code> or <code>shared-home</code> (or both), for example:</p> <pre><code>volumes:\nlocalHome:\ncustomVolume:\nhostPath:\npath: /path/to/my/data\nsharedHome:\ncustomVolume:\nnfs:\nserver: mynfsserver\npath: /export/path\n</code></pre> <p>Generally, any valid Kubernetes volume resource definition can be substituted here. However, as mentioned previously, externalising the volume definitions using <code>PersistentVolumes</code> is the strongly recommended approach.</p>"},{"location":"userguide/CONFIGURATION/#volumes-examples","title":"Volumes examples","text":"<ol> <li>Bitbucket needs a dedicated NFS server providing persistence for a shared home. Prior to installing the Helm chart, a suitable NFS shared storage solution must be provisioned. The exact details of this resource will be highly site-specific, but you can use this example as a guide: Implementation of an NFS Server for Bitbucket.</li> <li>We have an example detailing how an existing EFS filesystem can be created and consumed using static provisioning: Shared storage - utilizing AWS EFS-backed filesystem.</li> <li>You can also refer to an example on how a Kubernetes cluster and helm deployment can be configured to utilize AWS EBS backed volumes: Local storage - utilizing AWS EBS-backed volumes.</li> </ol>"},{"location":"userguide/CONFIGURATION/#additional-volumes","title":"Additional volumes","text":"<p>In addition to the <code>local-home</code> and <code>shared-home</code> volumes that are always attached to the product pods, you can attach your own volumes for your own purposes, and mount them into the product container.  Use the <code>additional</code> (under <code>volumes</code>) and <code>additionalVolumeMounts</code> values to both attach the volumes and mount them in to the product container.</p> <p>This might be useful if, for example, you have a custom plugin that requires its own filesystem storage.</p> <p>Example:</p> <pre><code>jira:\nadditionalVolumeMounts:\n- volumeName: my-volume\nmountPath: /path/to/mount\nvolumes:\nadditional:\n- name: my-volume\npersistentVolumeClaim:\nclaimName: my-volume-claim\n</code></pre>"},{"location":"userguide/CONFIGURATION/#database-connectivity","title":"Database connectivity","text":"<p>The products need to be supplied with the information they need to connect to the database service. Configuration for each product is mostly the same, with some small differences.</p>"},{"location":"userguide/CONFIGURATION/#databaseurl","title":"<code>database.url</code>","text":"<p>All products require the JDBC URL of the database. The format if this URL depends on the JDBC driver being used, but some examples are:</p> Vendor JDBC driver class Example JDBC URL PostgreSQL <code>org.postgresql.Driver</code> <code>jdbc:postgresql://&lt;dbhost&gt;:5432/&lt;dbname&gt;</code> MySQL <code>com.mysql.jdbc.Driver</code> <code>jdbc:mysql://&lt;dbhost&gt;/&lt;dbname&gt;</code> SQL Server <code>com.microsoft.sqlserver.jdbc.SQLServerDriver</code> <code>jdbc:sqlserver://&lt;dbhost&gt;:1433;databaseName=&lt;dbname&gt;</code> Oracle <code>oracle.jdbc.OracleDriver</code> <code>jdbc:oracle:thin:@&lt;dbhost&gt;:1521:&lt;SID&gt;</code> <p>Database creation</p> <p>The Atlassian product doesn't automatically create the database,<code>&lt;dbname&gt;</code>, in the <code>JDBC URL</code>, so you need to manually create a user and database for the used database instance. Details on how to create product-specific databases can be found below:</p> JiraConfluenceBitbucketBambooCrowd <p>Connect Jira to an external database</p> <p>Connect Confluence to an external database</p> <p>Connect Bitbucket to an external database</p> <p>Connect Bamboo to an external database</p> <p>Connect Crowd to an external database</p>"},{"location":"userguide/CONFIGURATION/#databasedriver","title":"<code>database.driver</code>","text":"<p>Jira and Bitbucket require the JDBC driver class to be specified (Confluence and Bamboo will autoselect this based on the <code>database.type</code> value, see below). The JDBC driver must correspond to the JDBC URL used; see the table above for example driver classes.</p> <p>Note that the products only ship with certain JDBC drivers installed, depending on the license conditions of those drivers.</p> <p>Non-bundled DB drivers</p> <p>MySQL and Oracle database drivers are not shipped with the products due to licensing restrictions. You will need to provide <code>additionalLibraries</code> configuration.</p>"},{"location":"userguide/CONFIGURATION/#databasetype","title":"<code>database.type</code>","text":"<p>Jira, Confluence and Bamboo all require this value to be specified, this declares the database engine to be used. The acceptable values for this include:</p> Vendor Jira Confluence Bamboo PostgreSQL <code>postgres72</code> <code>postgresql</code> <code>postgresql</code> MySQL <code>mysql57</code> / <code>mysql8</code> <code>mysql</code> <code>mysql</code> SQL Server <code>mssql</code> <code>mssql</code> <code>mssql</code> Oracle <code>oracle10g</code> <code>oracle</code> <code>oracle12c</code>"},{"location":"userguide/CONFIGURATION/#databasecredentials","title":"<code>database.credentials</code>","text":"<p>All products can have their database connectivity and credentials specified either interactively during first-time setup, or automatically by specifying certain configuration via Kubernetes.</p> <p>Depending on the product, the <code>database.type</code>, <code>database.url</code> and <code>database.driver</code> chart values can be provided. In addition, the database username and password can be provided via a Kubernetes secret, with the secret name specified with the <code>database.credentials.secretName</code> chart value. When all the required information is provided in this way, the database connectivity configuration screen will be bypassed during product setup.</p>"},{"location":"userguide/CONFIGURATION/#namespace","title":"Namespace","text":"<p>The Helm charts are not opinionated whether they have a Kubernetes namespace to themselves. If you wish, you can run multiple Helm releases of the same product in the same namespace.</p>"},{"location":"userguide/CONFIGURATION/#clustering","title":"Clustering","text":"<p>By default, the Helm charts will not configure the products for Data Center clustering. In order to enable clustering, the <code>enabled</code> property for clustering must be set to <code>true</code>.</p> <p>Clustering by default for Crowd</p> <p>Crowd does not offer clustering configuration via Helm Chart. Set <code>crowd.clustering.enabled</code> to <code>true/false</code> in <code>${CROWD_HOME}/shared/crowd.cfg.xml</code> and rollout restart Crowd StatefulSet after the initial product setup is complete.</p> JiraConfluenceBitbucketBambooCrowd <pre><code>jira:\nclustering:\nenabled: true\n</code></pre> <pre><code>confluence:\nclustering:\nenabled: true\n</code></pre> <pre><code>bitbucket:\nclustering:\nenabled: true\n</code></pre> <p>Because of the limitations outlined under Bamboo and clustering the <code>clustering</code> stanza is not available as a configurable property in the Bamboo <code>values.yaml</code>.</p> <p>Clustering is enabled by default. To disable clustering, set <code>crowd.clustering.enabled</code> to <code>false</code> in <code>${CROWD_HOME}/shared/crowd.cfg.xml</code> and rollout restart Crowd StatefulSet after the initial product setup is complete.</p> <p>In addition, the <code>shared-home</code> volume must be correctly configured as a ReadWriteMany (RWX) filesystem (e.g. NFS, AWS EFS and Azure Files)</p>"},{"location":"userguide/CONFIGURATION/#generating-configuration-files","title":"Generating configuration files","text":"<p>The Docker entrypoint scripts generate application configuration on first start; not all of these files are regenerated on subsequent starts. This is deliberate, to avoid race conditions or overwriting manual changes during restarts and upgrades. However, in deployments where configuration is purely specified through the environment (e.g. Kubernetes) this behaviour may be undesirable; this flag forces an update of all generated files.</p> <p>The affected files are: * Jira: <code>dbconfig.xml</code> * Confluence: <code>confluence.cfg.xml</code> * Bamboo: <code>bamboo.cfg.xml</code></p> <p>To force update of the configuration files when pods restart, set <code>&lt;product_name.forceConfigUpdate&gt;</code> to true. You can do it by passing an argument to helm install/update command: <pre><code>--set jira.forceConfigUpdate=true\n</code></pre> or set it in <code>values.yaml</code>:</p> <pre><code>jira:\n  forceConfigUpdate: true\n</code></pre>"},{"location":"userguide/CONFIGURATION/#additional-libraries-plugins","title":"Additional libraries &amp; plugins","text":"<p>The products' Docker images contain the default set of bundled libraries and plugins. Additional libraries and plugins can be mounted into the product containers during the Helm install. One such use case for this is mounting <code>JDBC</code> drivers that are not shipped with the products' by default.</p> <p>To make use of this mechanism, the additional files need to be available as part of a Kubernetes volume. Options here include putting them into the <code>shared-home</code> volume that's required as part of the prerequisites. Alternatively, you can create a custom <code>PersistenVolume</code> for them, as long as it has <code>ReadOnlyMany</code> capability.</p> <p>Custom volumes for loading libraries</p> <p>If you're not using the <code>shared-home</code> volume, then you can declare your own custom volume, by following the Additional volumes section above.</p> <p>You could even store the files as a <code>ConfigMap</code> that gets mounted as a volume, but you're likely to run into file size limitations there.</p> <p>Assuming that the existing <code>shared-home</code> volume is used for this, then the only configuration required is to specify the <code>additionalLibraries</code> in your <code>values.yaml</code> file, e.g.</p> <pre><code>jira:\nadditionalLibraries:\n- volumeName: shared-home\nsubDirectory: mylibs\nfileName: lib1.jar\n- volumeName: shared-home\nsubDirectory: mylibs\nfileName: lib2.jar\n</code></pre> <p>This will mount the <code>lib1.jar</code> and <code>lib2.jar</code> from the <code>mylibs</code> sub-directory from <code>shared-home</code> into the appropriate place in the container.</p> <p>Similarly, you can use <code>additionalBundledPlugins</code> to load product plugins into the container.</p> <p>System plugin</p> <p>Plugins installed via this method will appear as system plugins rather than user plugins. An alternative to this method is to install the plugins via \"Manage Apps\" in the product system administration UI.</p> <p>For more details on the above, and how 3rd party libraries can be supplied to a Pod see the example External libraries and plugins</p>"},{"location":"userguide/CONFIGURATION/#cpu-and-memory-requests","title":"CPU and memory requests","text":"<p>The Helm charts allow you to specify container-level CPU and memory resource requests and limits e.g.</p> <pre><code>jira:\nresources:\ncontainer:\nrequests:\ncpu: \"4\"\nmemory: \"8G\"\n</code></pre> <p>By default, the Helm Charts have no container-level resource limits, however there are default requests that are set.</p> <p>Specifying these values is fine for CPU limits/requests, but for memory resources it is also necessary to configure the JVM's memory limits. By default, the JVM maximum heap size is set to 1 GB, so if you increase (or decrease) the container memory resources as above, you also need to change the JVM's max heap size, otherwise the JVM won't take advantage of the extra available memory (or it'll crash if there isn't enough).</p> <p>You specify the JVM memory limits like this:</p> <pre><code>jira:\nresources:\njvm:\nmaxHeap: \"8g\"\n</code></pre> <p>Another difficulty for specifying memory resources is that the JVM requires additional overheads over and above the max heap size, and the container resources need to take account of that.  A safe rule-of-thumb would be for the container to request 2x the value of the max heap for the JVM.</p> <p>This requirement to configure both the container memory and JVM heap will hopefully be removed.</p> <p>You can read more about resource scaling and resource requests and limits.</p>"},{"location":"userguide/CONFIGURATION/#additional-containers","title":"Additional containers","text":"<p>The Helm charts allow you to add your own <code>container</code> and initContainer entries to the product pods. Use the <code>additionalContainers</code> and <code>additionalInitContainers</code> stanzas within the <code>values.yaml</code> for this. One use-case for an additional container would be to attach a sidecar container to the product pods.</p>"},{"location":"userguide/CONFIGURATION/#additional-options","title":"Additional options","text":"<p>The Helm charts also allow you to specify:</p> <ul> <li><code>additionalLabels</code></li> <li><code>tolerations</code>,</li> <li><code>nodeSelectors</code> </li> <li><code>affinities</code>.</li> </ul> <p>These are standard Kubernetes structures that will be included in the pods.</p>"},{"location":"userguide/INSTALLATION/","title":"Installation","text":"<p>Follow these instructions to install your Atlassian product using the Helm charts. Before you proceed with the installation, make sure you have followed the Prerequisites guide.</p>"},{"location":"userguide/INSTALLATION/#1-add-the-helm-chart-repository","title":"1. Add the Helm chart repository","text":"<p>Add the Helm chart repository to your local Helm installation:</p> <pre><code>helm repo add atlassian-data-center \\\nhttps://atlassian.github.io/data-center-helm-charts\n</code></pre> <p>Update the repository:</p> <pre><code>helm repo update\n</code></pre>"},{"location":"userguide/INSTALLATION/#2-obtain-valuesyaml","title":"2. Obtain <code>values.yaml</code>","text":"<p>Obtain the default product <code>values.yaml</code> file from the chart:</p> <pre><code>helm show values atlassian-data-center/&lt;product&gt; &gt; values.yaml\n</code></pre> <p>Bamboo deployments</p> <p>If deploying Bamboo, be sure to read about the current limitations relating to Bamboo deployments and values.yaml</p>"},{"location":"userguide/INSTALLATION/#3-configure-database","title":"3. Configure database","text":"<p>Crowd deployments</p> <p>Crowd Data Center Helm chart does not support unattended installation. Connection to the database must be manually configured during the product setup.</p> <p>Using the <code>values.yaml</code> file obtained in step 2, configure the usage of the database provisioned as part of the prerequisites. </p> <p>Automated setup steps</p> <p>By providing all the required database values, you will bypass the database connectivity configuration during the product setup.</p> <p>Migration</p> <p>If you are migrating an existing Data Center product to Kubernetes, use the values of your product's database. See Migration guide.</p> <p>Create a Kubernetes secret to store the connectivity details of the database:</p> <pre><code>kubectl create secret generic &lt;secret_name&gt; --from-literal=username='&lt;db_username&gt;' --from-literal=password='&lt;db_password&gt;'\n</code></pre> <p>Using the Kubernetes secret, update the <code>database</code> stanza within <code>values.yaml</code> appropriately. Refer to the commentary within the <code>values.yaml</code> file for additional details on how to configure the remaining database values:</p> <pre><code>database:\ntype: &lt;db_type&gt;\nurl: &lt;jdbc_url&gt;\ndriver: &lt;engine_driver&gt;\ncredentials:\nsecretName: &lt;secret_name&gt;\nusernameSecretKey: username\npasswordSecretKey: password\n</code></pre> <p>Database connectivity</p> <p>For additional information on how the above values should be configured, see the Database connectivity section of the configuration guide.</p> <p>Read about Kubernetes secrets.</p>"},{"location":"userguide/INSTALLATION/#4-configure-ingress","title":"4. Configure Ingress","text":"<p>Using the <code>values.yaml</code> file obtained in step 2, configure the Ingress controller provisioned as part of the Prerequisites. The values you provide here will be used to provision an Ingress resource for the controller. Refer to the associated comments within the <code>values.yaml</code> file for additional details on how to configure the Ingress resource:</p> <pre><code>ingress:\ncreate: true #1. Setting true here will create an Ingress resource\nnginx: true #2. If using the ingress-nginx controller set this property to true\nmaxBodySize: 250m\nhost: &lt;dns_host_name&gt; #2. Hosts can be precise matches (for example \u201cfoo.bar.com\u201d) or a wildcard (for example \u201c*.foo.com\u201d).\npath: \"/\"\nannotations:\ncert-manager.io/issuer: &lt;certificate_issuer&gt;\nhttps: true\ntlsSecretName: &lt;tls_certificate_name&gt;\n</code></pre> <p>Ingress configuration</p> <p>For additional details on Ingress controllers see the Ingress section of the configuration guide. </p> <p>See an example of how to set up a controller.</p>"},{"location":"userguide/INSTALLATION/#5-configure-persistent-storage","title":"5. Configure persistent storage","text":"<p>Using the <code>values.yaml</code> file obtained in step 2, configure the <code>shared-home</code> that was provisioned as part of the Prerequisites. See shared home example.</p> <p>If you are migrating an existing Data Center product to Kubernetes, use the values of your product's shared home. </p> <pre><code>volumes:\nsharedHome:\ncustomVolume:\npersistentVolumeClaim:\nclaimName: &lt;pvc_name&gt;\n</code></pre> <p>Each pod will also require its own <code>local-home</code> storage. This can be configured with a <code>StorageClass</code>, as can be seen in the local home example. Having created the <code>StorageClass</code>, update <code>values.yaml</code> to make use of it: </p> <pre><code>volumes:\nlocalHome:\npersistentVolumeClaim:\ncreate: true\nstorageClassName: &lt;storage-class-name&gt;\n</code></pre> <p>Volume configuration</p> <p>For more details, refer to the Volumes section of the configuration guide.</p> <p>Bitbucket shared storage</p> <p>Bitbucket needs a dedicated NFS server providing persistence for a shared home. Prior to installing the Helm chart, a suitable NFS shared storage solution must be provisioned. The exact details of this resource will be highly site-specific, but you can use this example as a guide: Implementation of an NFS Server for Bitbucket.</p>"},{"location":"userguide/INSTALLATION/#6-configure-clustering","title":"6. Configure clustering","text":"<p>By default, the Helm charts will not configure the products for Data Center clustering. You can enable clustering in the <code>values.yaml</code> file:</p> <pre><code>  clustering:\nenabled: true\n</code></pre> <p>Bamboo clustering</p> <p>Because of the limitations outlined under Bamboo and clustering the <code>clustering</code> stanza is not available as a configurable property in the Bamboo <code>values.yaml</code>.</p> <p>Crowd clustering</p> <p>Crowd does not offer clustering configuration via Helm Chart. Set <code>crowd.clustering.enabled</code> to <code>true/false</code> in <code>${CROWD_HOME}/shared/crowd.cfg.xml</code> and rollout restart Crowd StatefulSet after the initial product setup is complete.</p>"},{"location":"userguide/INSTALLATION/#7-configure-license","title":"7. Configure license","text":"<p>Pre-configuring license</p> <p>Pre-provisioning a license in this way is only applicable to <code>Confluence</code>, <code>Bitbucket</code> and <code>Bamboo</code> deployments. For <code>Jira</code> and <code>Crowd</code> deployments a license can be supplied via the setup wizard post deployment.</p> <p>You can configure the product license if you provide a <code>license</code> stanzas within the <code>values.yaml</code> obtained in step 2. To do that, create a Kubernetes secret to hold the product license:</p> <pre><code>kubectl create secret generic &lt;license_secret_name&gt; --from-literal=license-key='&lt;product_license_key&gt;'\n</code></pre> <p>Update the <code>values.yaml</code> file with the secrets:</p> <pre><code>license:\nsecretName: &lt;secret_name&gt;\nsecretKey: license-key\n</code></pre> Sysadmin credentials for Bitbucket and Bamboo  <p><code>Bitbucket</code> and <code>Bamboo</code> are slightly different from the other products in that they can be completely configured during deployment, meaning no manual setup is required. To do this, you need to update the <code>sysadminCredentials</code> and also provide the <code>license</code> stanza from the previous step.</p> <p>Create a Kubernetes secret to hold the Bitbucket/Bamboo system administrator credentials:</p> <pre><code>kubectl create secret generic &lt;sysadmin_creds_secret_name&gt; --from-literal=username='&lt;sysadmin_username&gt;' --from-literal=password='&lt;sysadmin_password&gt;' --from-literal=displayName='&lt;sysadmin_display_name&gt;' --from-literal=emailAddress='&lt;sysadmin_email&gt;'\n</code></pre> <p>Update the <code>values.yaml</code> file with the secrets:</p> <pre><code>sysadminCredentials:\nsecretName: &lt;sysadmin_creds_secret_name&gt;\nusernameSecretKey: username\npasswordSecretKey: password\ndisplayNameSecretKey: displayName\nemailAddressSecretKey: emailAddress\n</code></pre>"},{"location":"userguide/INSTALLATION/#8-configure-container-images","title":"8. Configure container images","text":"<p>By default, container images are pulled from official Atlassian DockerHub repositories. Deployments may also use 2 other official non-Altassian images - alpine and fluentd.</p> <p>In air-gapped environments that cannot directly access DockerHub you will need to pass custom repositories/tags to the Helm installation/upgrade command.</p>"},{"location":"userguide/INSTALLATION/#product-images","title":"Product images","text":"<p>For product images (such as Jira, Bitbucket, Confluence, Bamboo), update <code>values.yaml</code>:</p> <pre><code>image:\n  registry: artifactory.mycompany.com\n  repository: jira\n  tag: 7.8.20\n</code></pre>"},{"location":"userguide/INSTALLATION/#helper-containers","title":"Helper containers","text":"<p>If <code>volumes.sharedHome.persistentVolumeClaim.sharedHome.nfsPermissionFixer.enabled</code> is set to <code>true</code>, update <code>values.yaml</code>:</p> <pre><code>volumes:\n  sharedHome:\n    nfsPermissionFixer:\n      imageRepo: artifactory.mycompany.com/alpine\n      imageTag: latest\n</code></pre> <p>If <code>fluentd.enabled</code> is set to <code>true</code> (false by default), update <code>values.yaml</code>:</p> <pre><code>fluentd:\n  imageRepo: artifactory.mycompany.com/fluentd-kubernetes-daemonset\n  imageTag: v1.11.5-debian-elasticsearch7-1.2\n</code></pre>"},{"location":"userguide/INSTALLATION/#9-install-your-chosen-product","title":"9. Install your chosen product","text":"<pre><code>helm install &lt;release-name&gt; \\\natlassian-data-center/&lt;product&gt; \\\n--namespace &lt;namespace&gt; \\\n--version &lt;chart-version&gt; \\\n--values values.yaml\n</code></pre> <p>Values &amp; flags</p> <ul> <li><code>&lt;release-name&gt;</code> the name of your deployment. You can also use <code>--generate-name</code>.</li> <li> <p><code>&lt;product&gt;</code> the product to install. Options include: </p> <ul> <li><code>jira</code> </li> <li><code>confluence</code></li> <li><code>bitbucket</code></li> <li><code>bamboo</code></li> <li><code>bamboo-agent</code></li> <li><code>crowd</code></li> </ul> </li> <li> <p><code>&lt;namespace&gt;</code> optional flag for categorizing installed resources.</p> </li> <li><code>&lt;chart-version&gt;</code> optional flag for defining the chart version to be used. If omitted, the latest version of the chart will be used.</li> <li><code>values.yaml</code> optional flag for defining your site-specific configuration information. If omitted, the chart config default will be used.</li> <li>Add <code>--wait</code> if you wish the installation command to block until all of the deployed Kubernetes resources are ready, but be aware that this may wait for several minutes if anything is mis-configured.</li> </ul> <p>Elasticsearch for Bitbucket</p> <p>We highly recommend you use an external Elasticsearch installation for Bitbucket. When you run more than one node you need to have a separate Elasticsearch cluster to enable code search. See Bitbucket Elasticsearch recommendations.    </p>"},{"location":"userguide/INSTALLATION/#9-test-your-deployed-product","title":"9. Test your deployed product","text":"<p>Make sure the service pod/s are running, then test your deployed product:</p> <pre><code>helm test &lt;release-name&gt; --logs --namespace &lt;namespace&gt;\n</code></pre> <ul> <li>This will run some basic smoke tests against the deployed release.</li> <li>If any of these tests fail, it is likely that the deployment was not successful. Check the status of the deployed resources for any obvious errors that may have caused the failure.</li> </ul>"},{"location":"userguide/INSTALLATION/#10-complete-product-setup","title":"10. Complete product setup","text":"<p>Using the service URL provided by Helm post install, open your product in a web browser and complete the setup via the setup wizard. </p>"},{"location":"userguide/INSTALLATION/#11-additional-deployments","title":"11. Additional deployments","text":"<p>Bamboo agents and Bitbucket mirrors can also be deployed via their dedicated charts:</p> Bamboo agentBitbucket mirror <p>Bamboo agent installation</p> <p>Instructions for deploying Bamboo agents</p> <p>Bitbucket mirror installation</p> <p>Instructions for deploying Bitbucket mirror's</p>"},{"location":"userguide/INSTALLATION/#uninstall","title":"Uninstall","text":"<p>The deployment and all of its associated resources can be un-installed with the following command: <pre><code>helm uninstall &lt;release-name&gt; atlassian-data-center/&lt;product&gt;\n</code></pre></p>"},{"location":"userguide/MIGRATION/","title":"Migration","text":"<p>If you already have an existing Data Center product deployment, you can migrate it to a Kubernetes cluster using the Data Center Helm charts. </p> <p>You will need to migrate your database and your shared home (including local home for Bamboo), then all you need to do is to follow the Installation guide, using your migrated resources instead of provisioning new ones.</p>"},{"location":"userguide/MIGRATION/#migrating-your-database","title":"Migrating your database","text":"<p>To migrate your database, you should point the Helm charts to the existing database or to a migrated version of the database. Do this by updating the <code>database</code> stanza in the <code>values.yaml</code> file as explained in the Configure database step in the installation guide.</p>"},{"location":"userguide/MIGRATION/#migrating-your-shared-home","title":"Migrating your shared home","text":"<p>Application nodes should have access to a shared directory in the same path. Examples of what the shared file system stores include plugins, shared caches, repositories, attachments, and avatars. Configure your shared home by updating the <code>sharedHome</code> stanza in the <code>values.yaml</code> file as explained in the Configure persistent storage step in the installation guide.</p>"},{"location":"userguide/MIGRATION/#migrating-bamboo-server-local-home","title":"Migrating Bamboo server local home","text":"<p>Bamboo DC stores pertinent config data in local home, namely <code>bamboo.cfg.xml</code>. Care should be taken to include this data when migrating Bamboo DC deployments.</p>"},{"location":"userguide/MIGRATION/#helpful-links","title":"Helpful links","text":"<ul> <li>Atlassian Data Center migration plan\u00a0- gives\u00a0some guidance on overall process, organizational preparedness, estimated time frames, and app\u00a0compatibility.\u00a0</li> <li>Atlassian Data Center migration checklist\u00a0- also provides useful tests and checks to perform throughout the moving process.</li> <li>Migrating to another database - describes how to migrate your data from your existing database to another database:<ul> <li>Migrating Confluence to another database</li> <li>Migrating Jira to another database </li> <li>Migrating Bamboo to another database </li> </ul> </li> </ul> <p>Availability Zone proximity</p> <p>For better performance consider co-locating your migrated database in the same Availability Zone (AZ) as your product nodes. Database-heavy operations, such as full re-index, become significantly faster when the database is collocated with the Data Center node in the same AZ. However we don't recommend this if you're running critical workloads.</p>"},{"location":"userguide/OPERATION/","title":"Operation","text":"<p>Once you have installed your product, use this document if you want to scale your product, update your product, or see what examples we have.</p>"},{"location":"userguide/OPERATION/#managing-resources","title":"Managing resources","text":"<p>You can scale your application by adding additonal pods or by managing available resources with requests and limits.</p>"},{"location":"userguide/OPERATION/#upgrading-application","title":"Upgrading application","text":""},{"location":"userguide/OPERATION/#kubernetes-update-strategies","title":"Kubernetes update strategies","text":"<p>Kubernetes provides two strategies to update applications managed by <code>statefulset</code> controllers:</p>"},{"location":"userguide/OPERATION/#rolling-update","title":"Rolling update","text":"<p>The pods will be upgraded one by one until all pods run containers with the updated template. The upgrade is managed by  Kubernetes and the user has limited control during the upgrade process, after having modified the template. This is the default  upgrade strategy in Kubernetes. </p> <p>To perform a canary or multi-phase upgrade, a partition can be defined on the cluster and Kubernetes will upgrade just  the nodes in that partition. </p> <p>The default implementation is based on RollingUpdate strategy with no partition defined. </p>"},{"location":"userguide/OPERATION/#ondelete-strategy","title":"OnDelete strategy","text":"<p>In this strategy users select the pod to upgrade by deleting it, and Kubernetes will replace it by creating a new pod  based on the updated template. To select this strategy the following should be replaced with the current   implementation of <code>updateStrategy</code> in the <code>statefulset</code> spec:</p> <pre><code>  updateStrategy:\ntype: OnDelete\n</code></pre>"},{"location":"userguide/OPERATION/#upgrade","title":"Upgrade","text":"<ul> <li>To learn about upgrading the Helm charts see Helm chart upgrade.  </li> <li>To learn about upgrading the products without upgrading the Helm charts see Products upgrade.</li> </ul>"},{"location":"userguide/OPERATION/#examples","title":"Examples","text":""},{"location":"userguide/OPERATION/#logging","title":"Logging","text":""},{"location":"userguide/OPERATION/#how-to-deploy-an-efk-stack-to-kubernetes","title":"How to deploy an EFK stack to Kubernetes","text":"<p>There are different methods to deploy an EFK stack. We provide two deployment methods, the first is deploying EFK locally on Kubernetes, and the second is using managed Elasticsearch outside the Kubernetes cluster. Please refer to Logging in Kubernetes.</p>"},{"location":"userguide/PREREQUISITES/","title":"Prerequisites","text":""},{"location":"userguide/PREREQUISITES/#requirements","title":"Requirements","text":"<p>In order to deploy Atlassian\u2019s Data Center products, the following is required:</p> <ol> <li>An understanding of Kubernetes and Helm concepts.</li> <li><code>kubectl</code> <code>v1.21</code> or later, must be compatible with your cluster.</li> <li><code>helm</code> <code>v3.3</code> or later.</li> </ol>"},{"location":"userguide/PREREQUISITES/#environment-setup","title":"Environment setup","text":"<p>Before installing the Data Center Helm charts you need to set up your environment:</p> <ol> <li>Create and connect to the Kubernetes cluster</li> <li>Provision an Ingress Controller</li> <li>Provision a database</li> <li>Configure a shared-home volume</li> <li>Configure a local-home volume</li> </ol> <p>Elasticsearch for Bitbucket</p> <p>We highly recommend you use an external Elasticsearch installation for Bitbucket. When you run more than one node you need to have a separate Elasticsearch cluster to enable code search. See Bitbucket Elasticsearch recommendations.</p>"},{"location":"userguide/PREREQUISITES/#create-and-connect-to-the-kubernetes-cluster","title":"Create and connect to the Kubernetes cluster","text":"<ul> <li>In order to install the charts to your Kubernetes cluster (version 1.21+), your Kubernetes client config must be configured appropriately, and you must have the necessary permissions.</li> <li>It is up to you to set up security policies.</li> </ul> <p>See examples of provisioning Kubernetes clusters on cloud-based providers.</p>"},{"location":"userguide/PREREQUISITES/#provision-an-ingress-controller","title":"Provision an Ingress Controller","text":"<ul> <li>This step is necessary in order to make your Atlassian product available from outside of the Kubernetes cluster after deployment.</li> <li>The Kubernetes project supports and maintains ingress controllers for the major cloud providers including; AWS, GCE and nginx. There are also a number of open-source third-party projects available.</li> <li>Because different Kubernetes clusters use different ingress configurations/controllers, the Helm charts provide\u00a0Ingress Object\u00a0templates only.</li> <li>The Ingress resource provided as part of the Helm charts is geared toward the\u00a0NGINX Ingress Controller\u00a0and can be configured via the\u00a0<code>ingress</code>\u00a0stanza in the appropriate\u00a0<code>values.yaml</code> (an alternative controller can be used).</li> <li>For more information about the Ingress controller go to the Ingress section of the configuration guide.</li> </ul> <p>See an example of provisioning an NGINX Ingress Controller.</p>"},{"location":"userguide/PREREQUISITES/#provision-a-database","title":"Provision a database","text":"<ul> <li>Must be of a type and version supported by the Data Center product you wish to install:</li> </ul> JiraConfluenceBitbucketBambooCrowd <p>Supported databases</p> <p>Supported databases</p> <p>Supported databases</p> <p>Supported databases</p> <p>Supported databases</p> <ul> <li>Must be reachable from the product deployed within your Kubernetes cluster.</li> <li>The database service may be deployed within the same Kubernetes cluster as the Data Center product or elsewhere.</li> <li>The products need to be provided with the information they need to connect to the database service. Configuration for each product is mostly the same, with some small differences. For more information go to the Database connectivity section of the configuration guide.</li> </ul> <p>Reducing pod to database latency</p> <p>For better performance consider co-locating your database in the same Availability Zone (AZ) as your product nodes. Database-heavy operations, such as full re-index, become significantly faster when the database is collocated with the Data Center node in the same AZ. However we don't recommend this if you're running critical workloads.</p> <p>See an example of provisioning databases on cloud-based providers.</p>"},{"location":"userguide/PREREQUISITES/#configure-a-shared-home-volume","title":"Configure a shared-home volume","text":"<ul> <li>All of the Data Center products require a shared network filesystem if they are to be operated in multi-node clusters. If no shared filesystem is available, the products can only be operated in single-node configuration.</li> <li>Some cloud based options for a shared filesystem include AWS EFS and Azure Files. You can also stand up your own NFS</li> <li>The logical representation of the chosen storage type within Kubernetes can be defined as <code>PersistentVolumes</code> with an associated <code>PersistentVolumeClaims</code> in a <code>ReadWriteMany (RWX)</code> access mode.</li> <li>For more information about volumes see the Volumes section of the configuration guide.</li> </ul> <p>See examples of creating shared storage.</p>"},{"location":"userguide/PREREQUISITES/#configure-local-home-volume","title":"Configure local-home volume","text":"<ul> <li>As with the shared-home, each pod requires its own volume for <code>local-home</code>. Each product needs this for defining operational data.</li> <li>If not defined, an emptyDir will be utilised.</li> <li>Although an <code>emptyDir</code> may be acceptable for evaluation purposes, we recommend that each pod is allocated its own volume.</li> <li>A <code>local-home</code> volume could be logically represented within the cluster using a <code>StorageClass</code>. This will dynamically provision an AWS EBS volume to each pod.</li> </ul> <p>An example of this strategy can be found the local storage example.</p>"},{"location":"userguide/VERIFICATION/","title":"Verification","text":"<p>From release 1.11.0, all the Helm charts are signed with a GPG key, following the instructions on the official Helm documentation. </p> <p>To verify the integrity of the charts,  1. Download chart <code>.tgz</code> file, <code>.prov</code> file and <code>helm_key.pub</code> from release assets, </p> <ol> <li> <p>Import the public key into your local GPG keyring. (Install GnuPG tool if you haven't done so already.) </p> <pre><code>gpg --import helm_key.pub </code></pre> </li> <li> <p>At present, Helm only supports the legacy gpg format so export the keyring into the legacy format:     <pre><code>gpg --export &gt;~/.gnupg/pubring.gpg\n</code></pre></p> </li> <li> <p>Verify the chart.     <pre><code>helm verify /path/to/product.tgz \n</code></pre></p> </li> </ol> <p>If the verification is successful, the output would be something like:  <pre><code>helm verify ~/Downloads/jira-1.11.0.tgz                                                                         \nSigned by: Atlassian DC Deployments &lt;dc-deployments@atlassian.com&gt;\nUsing Key With Fingerprint: DD1A5B2F7A599129274FB10AD38C66448E19B403\nChart Hash Verified: sha256:ca102cbf416a5c87998d06ba4527b5afc99e1d7d1776317ddd07720251715fde\n</code></pre></p>"},{"location":"userguide/resource_management/JIRA_INDEX_SNAPSHOT/","title":"Creating an initial index snapshot in Jira","text":"<p>These steps should be followed to enable shared index snapshots with Jira:</p> <ol> <li>Log into the Jira instance as the Administrator</li> <li>Go to <code>Settings</code> -&gt; <code>System</code> -&gt; <code>Indexing</code></li> <li>There should be no errors on this page i.e. </li> <li>If there are errors (as seen below) perform a <code>Full re-index</code> before proceeding </li> <li>Once the <code>Full re-index</code> is complete, scroll down to <code>Index Recovery</code> settings visible on the same page </li> <li>Take note of the current settings</li> <li>Temporarily change these values (<code>Edit Settings</code>), as depicted in the screenshot below. The cron expression will create an index snapshot every minute </li> <li>Wait for the snapshot to be created, by checking for an archive in <code>&lt;shared-home&gt;/export/indexsnapshots</code></li> <li>When the snapshot is available, revert the settings noted in <code>step 6</code>, or back to the defaults: </li> <li>Consider keeping the <code>Enable index recovery</code> setting so that it is set to <code>ON</code></li> <li>Proceed with scaling the cluster as necessary</li> </ol>"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/","title":"Resource requests and limits","text":"<p>To ensure that Kubernetes appropriately schedules resources, the respective product <code>values.yaml</code> is configured with default <code>cpu</code> and <code>memory</code> resource request values .</p>"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/#resource-requests","title":"Resource requests","text":"<p>The default resource requests that are used for each product are defined below. </p> <p>Note: these values are geared towards small data sets. For larger enterprise deployments refer to the Data Center infrastructure recommendations.</p> <p>Using the formula below, the <code>memory</code> specific values are derived from the default <code>JVM</code> requirements defined for each product's Docker container.</p> Product CPU Memory Jira <code>2</code> <code>2G</code> Confluence <code>2</code> <code>2G</code> Bitbucket <code>2</code> <code>2G</code> Bamboo <code>2</code> <code>2G</code> Bamboo agent <code>1</code> <code>2G</code> Crowd <code>2</code> <code>1G</code>"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/#synchrony-container-resources","title":"Synchrony container resources","text":"<p>A Confluence cluster requires a Synchrony container. The default resource requests that are used for the Synchrony container are:</p> <ul> <li><code>cpu: 2</code></li> <li><code>memory: 2.5G</code> </li> </ul>"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/#memory-request-sizing","title":"Memory request sizing","text":"<p>Request sizing must allow for the size of the product <code>JVM</code>. That means the <code>maximum heap size</code>, <code>minumum heap size</code> and the <code>reserved code cache size</code> (if applicable) plus other JVM overheads, must be considered when defining the request <code>memory</code> size. As a rule of thumb the formula below can be used to deduce the appropriate request memory size. <pre><code>(maxHeap + codeCache) * 1.5\n</code></pre></p>"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/#resource-limits","title":"Resource limits","text":"<p>Environmental and hardware constraints are different for each deployment, therefore the product's <code>values.yaml</code> does not provide a resource <code>limit</code> definition. Resource usage limits can be defined by updating the commented out the <code>resources.container.limits</code> stanza within the appropriate <code>values.yaml</code>. For example:</p> <pre><code>container:\nlimits:\ncpu: \"2\"\nmemory: \"4G\"\nrequests:\ncpu: \"2\" memory: \"2G\"\n</code></pre>"},{"location":"userguide/resource_management/RESOURCE_SCALING/","title":"Product scaling","text":"<p>For optimum performance and stability the appropriate resource <code>requests</code> and <code>limits</code> should be defined for each pod. The number of pods in the product cluster should also be carefully considered. Kubernetes provides means for horizontal and vertical scaling of the deployed pods within a cluster, these approaches are described below.</p>"},{"location":"userguide/resource_management/RESOURCE_SCALING/#horizontal-scaling-adding-pods","title":"Horizontal scaling - adding pods","text":"<p>The Helm charts provision one <code>StatefulSet</code> by default. The number of replicas within this <code>StatefulSet</code> can be altered either declaratively or imperatively. Note that the Ingress must support cookie-based session affinity in order for the products to work correctly in a multi-node configuration.</p> DeclarativelyImperatively <ol> <li>Update <code>values.yaml</code> by modifying the <code>replicaCount</code> appropriately.</li> <li>Apply the patch:   <pre><code>helm upgrade &lt;release&gt; &lt;chart&gt; -f &lt;values file&gt;\n</code></pre></li> </ol> <pre><code>kubectl scale statefulsets &lt;statefulsetset-name&gt; --replicas=n\n</code></pre> <p>Initial cluster size</p> <p>Jira, Confluence, and Crowd all require manual configuration after the first pod is deployed and before scaling up to additional pods, therefore when you deploy the product only one pod (replica) is created. The initial number of pods that should be started at deployment of each product is set in the <code>replicaCount</code> variable found in the values.yaml and should always be kept as 1.</p> <p>Bamboo cluster size</p> <p>Bamboo server currently has limitations relating to clustering, as such, unlike the other products Bamboo server can only be scaled to a maximum of <code>1</code> pod.</p> <p>For details on modifying the <code>cpu</code> and <code>memory</code> requirements of the <code>StatefulSet</code> see section Vertical Scaling below. Additional details on the resource requests and limits used by the <code>StatfulSet</code> can be found in Resource requests and limits.</p>"},{"location":"userguide/resource_management/RESOURCE_SCALING/#scaling-jira-safely","title":"Scaling Jira safely","text":"<p>At present there are issues relating to index replication with Jira when immediately scaling up by more than 1 pod at a time. See Jira and horizontal scaling.</p> <p>Before scaling your cluster</p> <p>Make sure there's at least one snapshot file in the <code>&lt;shared-home&gt;/export/indexsnapshots</code> directory. New pods will attempt to use the files in this directory to replicate the index. If there is no snapshot present in  <code>&lt;shared-home&gt;/export/indexsnapshots</code> then create an initial index snapshot</p> <p>Having followed the steps above, and ensured a healthy snapshot index is available, scale the cluster as necessary. Once scaling is complete confirm that the index is still healthy using the approach prescribed in Step 3. If there are still indexing issues then please refer to the guides below for details on how address them:</p> <ul> <li>Unable to perform a background re-index error</li> <li>Troubleshoot index problems in Jira server</li> </ul>"},{"location":"userguide/resource_management/RESOURCE_SCALING/#vertical-scaling-adding-resources","title":"Vertical scaling - adding resources","text":"<p>The resource <code>requests</code> and <code>limits</code> for a <code>StatefulSet</code> can be defined before product deployment or for deployments that are already running within the Kubernetes cluster. Take note that vertical scaling will result in the pod being re-created with the updated values.</p>"},{"location":"userguide/resource_management/RESOURCE_SCALING/#prior-to-deployment","title":"Prior to deployment","text":"<p>Before performing a helm install update the appropriate products <code>values.yaml</code> <code>container</code> stanza with the desired <code>requests</code> and <code>limits</code> values i.e.  <pre><code> container: limits:\ncpu: \"4\"\nmemory: \"4G\"\nrequests:\ncpu: \"2\"\nmemory: \"2G\"\n</code></pre></p>"},{"location":"userguide/resource_management/RESOURCE_SCALING/#post-deployment","title":"Post deployment","text":"<p>For existing deployments the <code>requests</code> and <code>limits</code> values can be dynamically updated either declaratively or imperatively </p> DeclarativelyImperatively <p>This the preferred approach as it keeps the state of the cluster, and the helm charts themselves in sync.</p> <ol> <li>Update <code>values.yaml</code> appropriately</li> <li>Apply the patch:</li> </ol> <pre><code>helm upgrade &lt;release&gt; &lt;chart&gt; -f &lt;values file&gt;\n</code></pre> <p>Using <code>kubectl edit</code> on the appropriate <code>StatefulSet</code> the respective <code>cpu</code> and <code>memory</code> values can be modified i.e.</p> <pre><code>resources:\nrequests:\ncpu: \"2\"\nmemory: 2G\n</code></pre> <p>Saving the changes will then result in the existing product pod(s) being re-provisioned with the updated values.</p>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/","title":"Helm chart upgrade","text":"<p>Each Helm chart has a default product version that might change in next Helm chart  version. So be aware that if you upgrade the Helm chart, it might lead to upgrading the product as well. This depends on the current and target Helm chart versions. </p> <p>Do you want to upgrade the product to a new version?</p> <p>If you want to upgrade the product version without upgrading the Helm chart then   refer to Product upgrades. </p> <p>Before upgrading the Helm chart, first consider: </p> <ul> <li>the version of the current Helm chart</li> <li>the version of the product running on your Kubernetes cluster</li> <li>the target version of the Helm chart you want to upgrade to</li> <li>the target version of the product you want to upgrade to</li> </ul> <p>You need to know if the target product version is zero-downtime compatible (if it is subject to change).  Based on this information you may need to choose a different upgrade method.</p> <p>Upgrade product strategies</p> <p>There are two options for upgrade:</p> <ul> <li>Normal upgrade: The service will have interruptions during the upgrade.</li> <li>Rolling upgrade: The upgrade will proceed with zero downtime.</li> </ul>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#1-find-the-current-version-of-the-installed-helm-chart","title":"1. Find the current version of the installed Helm chart","text":"<p>To find the current version of Helm chart and the product version run the following command:</p> <pre><code>helm list --namespace &lt;namespace&gt; </code></pre> <p>You can see the current Helm chart tag version in the <code>CHART</code> column, and the current product tag version in the <code>APP VERSION</code>  column for each release name. </p>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#2-define-the-target-helm-chart-version","title":"2. Define the target Helm chart version","text":"<p>Do you have the Atlassian Helm chart repository locally?</p> <p>Make sure you have the Atlassian Helm chart repository in your local Helm repositories. Run the following command to add them:</p> <pre><code>helm repo add atlassian-data-center \\\nhttps://atlassian.github.io/data-center-helm-charts\n</code></pre> <p>Update the Helm chart repository on your local Helm installation:</p> <pre><code>helm repo update\n</code></pre> <p>Target Helm chart version</p> <p>The target Helm chart version must be higher than the current Helm chart version.</p> <p>To see all available Helm chart versions of the specific product run this command:</p> <pre><code>helm search repo atlassian-data-center/&lt;product&gt; --versions\n</code></pre> <p>Select the target Helm chart version. You can find the default application version (target product version tag)   in the <code>APP VERSION</code> column.</p> <p>Upgrading the Helm chart to a MAJOR version is not backward compatible.</p> <p>The Helm chart is semantically versioned. You need to take some extra  steps if you are upgrading the Helm chart to a MAJOR version. Before you proceed, learn about the steps for your   target version in the upgrading section. </p>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#3-define-the-upgrade-method","title":"3. Define the upgrade method","text":"<p>Considering the current and target product versions there are different scenarios: </p> <ol> <li>The versions are different, and the target product version is not zero-downtime compatible.  </li> <li>The versions are different, and the target product version is zero-downtime compatible.  </li> <li>The versions are the same.</li> </ol> <p>See the following links to find out if two versions of a product are zero-downtime compatible</p> <ul> <li>Jira: Upgrading Jira with zero downtime </li> <li>Confluence: Upgrading Confluence with zero downtime </li> <li>Bitbucket: Upgrading Bitbucket with zero downtime</li> <li>Bamboo: Zero downtime upgrades for Bamboo server and Bamboo agents are currently not supported.</li> <li>Crowd: Zero downtime upgrades for Crowd are currently not supported.</li> </ul> <p>All supported Jira versions are zero-downtime compatible</p> <p>The minimum supported version of Jira in the Data Center Helm Charts is <code>8.19</code>.   Considering any Jira version later than 8.x is zero-downtime compatible, all supported Jira Data Center versions   are zero-downtime compatible. </p> <p>Based on the scenario follow one of these options in the next step:</p> <ul> <li>Normal upgrade: Upgrade Helm chart when the target product version is not zero-downtime compatible, or you want  to avoid mixed version during the upgrade. In this option the product will have a downtime during the upgrade process. </li> <li>Rolling upgrade: Upgrade Helm chart when the target product version is zero-downtime compatible. This option  will only apply when the target product version is zero-downtime compatible. If you are not sure about this see the links above. </li> <li>No product upgrade: Upgrade the Helm chart with no change in product version. We recommend this method when the target product version is the same as the current product version, or for any other reason you may not want to change the product version but still upgrade the helm chart. </li> </ul>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#4-get-the-current-helm-release-values","title":"4. Get the current Helm release values","text":"<p>--reuse-values argument</p> <p>Do not pass <code>--reuse-values</code> to <code>helm upgrade</code> command because the default values in the new Helm chart version will be ignored which can result in templating errors and upgrade failures.</p> <p>If you have not saved the original values file used during the initial Helm chart installation, you can retrieve user-supplied values:</p> <pre><code>helm get values &lt;release-name&gt; -n &lt;namespace&gt; -o yaml &gt; values.yaml\n</code></pre> <p>Use this file in the argument to <code>helm upgrade</code> command, for example:</p> <pre><code>-f /home/user/values.yaml\n</code></pre>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#5-upgrade-the-helm-chart","title":"5. Upgrade the Helm chart","text":"<p>Tip: Monitor the pods during the upgrade process</p> <p>You can monitor the pod activities during the upgrade by running the following command in a separate terminal:   <pre><code>kubectl get pods --namespace &lt;namespace&gt; --watch\n</code></pre></p> Normal upgradeRolling upgradeUpgrade with no change in the product version"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#helm-chart-upgrade-with-downtime","title":"Helm chart upgrade with downtime","text":"<p>You need to use this method to upgrade the Helm chart if:</p> <ul> <li>the target product version is not zero downtime-compatible</li> <li>for any other reason you would prefer to avoid running the cluster in mix mode</li> </ul> <p>Upgrading the Helm chart might change the product version</p> <p>If you want to upgrade the Helm chart to a newer version but don't want to change  the product version then follow the Upgrade with no change in product version tab.</p> <p>The strategy for upgrading the product with downtime is to scale down the cluster to zero nodes and then  start the nodes with new product versions. And finally scale the cluster up to the original number of nodes.  Here are step-by-step instructions for the upgrade process:</p> <ol> <li>Find out the number of nodes in the cluster.     <pre><code>kubectl describe sts &lt;release-name&gt; --namespace &lt;namespace&gt; | grep 'Replicas'\n</code></pre></li> <li> <p>Upgrade the Helm chart.     Replace the product name in the following command:     <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/&lt;product&gt; \\\n--version &lt;target-helm-chart-version&gt; \\\n-f &lt;path-to-values-yaml&gt; \\\n--set replicaCount=1 \\\n--wait \\\n--namespace &lt;namespace&gt;\n</code></pre>     The cluster will scale down to zero nodes. Then one pod with the target product version will be recreated      and join the cluster. </p> </li> <li> <p>Scale up the cluster.     After you confirm the new pod is in <code>Running</code> status then scale up the cluster to the same number      of nodes as before the upgrade:      <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/confluence \\\n-f &lt;path-to-values-yaml&gt; \\\n--set replicaCount=&lt;n&gt; \\\n--wait \\\n--namespace &lt;namespace&gt;\n</code></pre></p> </li> </ol>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#helm-chart-upgrade-with-zero-downtime","title":"Helm chart upgrade with zero downtime","text":"<p>Upgrade the Helm chart might change the product version</p> <p>If you want to upgrade the Helm chart to newer version but don't want to change  the product version then follow the Upgrade with no change in product version tab.</p> <p>Rolling upgrade is not possible if the cluster has only one node</p> <p>If you have just one node in the cluster then you can't take advantage of the zero-downtime approach. You may   scale up the cluster to at least two nodes before upgrading or there will be a downtime during the upgrade. </p> <p>In order to upgrade the Helm chart when the target product version is different from the current product version,  you can use upgrade with zero downtime to avoid any service interruption during the upgrade. To use this option the target version must be zero-downtime compatible. </p> <p>Make sure the product target version is zero downtime-compatible</p> <p>To ensure you will have a smooth upgrade make sure the product target version is zero-downtime   compatible. If you still aren't sure about this go back to step 3. </p> <p>Here are the step-by-step instructions of the upgrade process. These steps may vary for each product:</p> JiraConfluenceBitbucketBambooCrowd <ol> <li> <p>Put Jira into upgrade mode.      Go to Administration &gt; Applications &gt; Jira upgrades and click Put Jira into upgrade mode.      </p> </li> <li> <p>Run the upgrade using Helm. </p> <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/jira \\\n--version &lt;target-helm-chart-version&gt; \\\n-f &lt;path-to-values-yaml&gt; \\\n--wait \\\n--namespace &lt;namespace&gt;\n</code></pre> </li> <li> <p>Wait for the upgrade to finish.      The pods will be recreated with the updated version, one at a time.</p> <p></p> </li> <li> <p>Finalize the upgrade.     After all the pods are active with the new version, click Run upgrade tasks to finalize the upgrade:</p> <p></p> </li> </ol> <ol> <li> <p>Put Confluence into upgrade mode.       From the admin page click on Rolling Upgrade and set the Confluence in Upgrade mode:</p> <p></p> </li> <li> <p>Run the upgrade using Helm.       <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/confluence \\\n--version &lt;target-helm-chart-version&gt; \\\n-f &lt;path-to-values-yaml&gt; \\\n--wait \\\n--namespace &lt;namespace&gt;\n</code></pre>      Wait until all pods are recreated and are back to <code>Running</code> status. </p> </li> <li> <p>Wait for the upgrade to finish.       The pods will be recreated with the updated version, one at a time.</p> <p></p> </li> <li> <p>Finalize the upgrade.       After all the pods are active with the new version, click Run upgrade tasks to finalize the upgrade:</p> <p></p> </li> </ol> <ol> <li> <p>Put Bitbucket into upgrade mode.      From the admin page click on Rolling Upgrade and set the Bitbucket in Upgrade mode:</p> <p></p> </li> <li> <p>Run the upgrade using Helm. </p> <p><pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/bitbucket \\\n--version &lt;target-helm-chart-version&gt; \\\n-f &lt;path-to-values-yaml&gt; \\\n--wait \\\n--namespace &lt;namespace&gt;\n</code></pre> Wait until all pods are recreated and are back to <code>Running</code> status. </p> </li> <li> <p>Wait for the upgrade to finish.      The pods will be recreated with the updated version, one at a time.</p> <p></p> </li> <li> <p>Finalize the upgrade.      After all the pods are active with the new version, click Run upgrade tasks to finalize the upgrade:</p> <p></p> </li> </ol> <p>Bamboo and zero downtime upgrades</p> <p>Zero downtime upgrades for Bamboo server and Bamboo agents are currently not supported.</p> <p>Crowd and zero downtime upgrades</p> <p>Zero downtime upgrades for Crowd are currently not supported.</p>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#helm-chart-upgrade-with-no-change-in-product-version","title":"Helm chart upgrade with no change in product version","text":"<p>If your target Helm chart has a different product version in comparison with the current product version, and you  still want to keep the current product version unchanged, you should use the following command to upgrade the Helm chart:</p> <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/&lt;product&gt; \\\n--version &lt;helm-chart-target-version&gt; \\\n-f &lt;path-to-values-yaml&gt; \\\n--set image.tag=&lt;current-product-tag&gt; \\\n--wait \\\n--namespace &lt;namespace&gt;\n</code></pre> <p>However, when the product versions of target and current Helm charts are the same, then you can run the following command to upgrade the Helm chart only:</p> <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/&lt;product&gt; \\\n--version &lt;helm-chart-target-version&gt; \\\n-f &lt;path-to-values-yaml&gt; \\\n--wait \\\n--namespace &lt;namespace&gt;\n</code></pre>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/","title":"Products upgrade","text":"<p>We recommend upgrading the Helm chart rather than upgrading the product directly. However, if you want to upgrade the  product to a specific version that is not listed in the Helm charts, or if you don't want to upgrade Helm chart to a newer version but you still need to upgrade the product version, then you are in a right place.   </p> <p>To upgrade the product to a newer version without upgrading the Helm chart follow these steps:</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#1-find-the-tag-of-the-target-image","title":"1. Find the tag of the target image","text":"<p>Go to the Atlassian Docker Hub page of the relevant product to pick a tag that matches your target version.</p> <p>Atlassian Docker Hub page for supported products:</p> <ul> <li>Jira: atlassian/jira-software</li> <li>Confluence: atlassian/confluence-server</li> <li>Bitbucket: atlassian/bitbucket-server</li> <li>Bamboo: atlassian/bamboo</li> <li>Bamboo agent: atlassian/bamboo-agent-base</li> <li>Crowd: atlassian/crowd</li> </ul> <p>In the example you're running Jira using the <code>8.13.0-jdk11</code> tag, and you'll be upgrading to <code>8.13.1-jdk11</code> - our target.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#2-define-the-upgrade-strategy","title":"2. Define the upgrade strategy","text":"<p>There are two strategies to upgrade the application:</p> <ul> <li>Normal upgrade: The service will have interruptions during the upgrade.</li> <li>Rolling upgrade: The upgrade will proceed with zero downtime.</li> </ul> <p>You can use rolling upgrade only if the target version is zero-downtime compatible. </p> <p>Can you use the rolling upgrade option?</p> <p>To confirm if you can run a rolling upgrade option, check your current and target product versions in the relevant link:  </p> <ul> <li>Jira: Upgrading Jira with zero downtime </li> <li>Confluence: Upgrading Confluence with zero downtime </li> <li>Bitbucket: Upgrading Bitbucket with zero downtime</li> <li>Bamboo: Zero downtime upgrades for Bamboo server and Bamboo agents are currently not supported.</li> </ul>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#3-upgrade-the-product","title":"3. Upgrade the product","text":"Normal UpgradeRolling upgrade"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#normal-upgrade","title":"Normal upgrade","text":"<p>The service will have interruptions during the normal upgrade</p> <p>You can use this method to upgrade the Helm chart if:</p> <ul> <li>The target product version is not zero-downtime compatible</li> <li>If you prefer to avoid running the cluster in mix mode</li> </ul> <p>The strategy for normal upgrading is to scale down the cluster to zero nodes, and then   start one node with the new product version. Then scale up the cluster to the original number of nodes.   Here are the step-by-step instructions for the upgrade process:</p> <ol> <li>Find out the current number of nodes.  Run the following command:   <pre><code>kubectl describe sts &lt;release-name&gt; --namespace &lt;namespace&gt; | grep 'Replicas'\n</code></pre></li> <li> <p>Run the upgrade using Helm.  Based on the product you want to upgrade replace the product name in the following command and run:  <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/&lt;product&gt; \\\n--reuse-values \\\n--set replicaCount=1 \\\n--set image.tag=&lt;target-tag&gt; \\\n--wait \\\n--namespace &lt;namespace&gt;\n</code></pre>  The cluster will scale down to zero nodes. Then one pod with the target product version will be recreated   and join the cluster. </p> </li> <li> <p>Scale up the cluster.  After you confirm the new pod is in <code>Running</code> status then scale up the cluster to the same number   of nodes as before the upgrade:   <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/confluence \\\n--reuse-values \\\n--set replicaCount=&lt;n&gt; \\\n--wait \\\n--namespace &lt;namespace&gt;\n</code></pre></p> </li> </ol>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#rolling-zero-downtime-upgrade","title":"Rolling (zero downtime) upgrade","text":"<p>Select the product tab to upgrade</p> <p>Upgrading the product with zero downtime is bit different for each product. Please select the product and follow  the steps to complete the rolling upgrade.  </p> JiraConfluenceBitbucketBambooCrowd <p>Bamboo and zero downtime upgrades</p> <p>Zero downtime upgrades for Bamboo server and Bamboo agents are currently not supported.</p> <p>Crowd and zero downtime upgrades</p> <p>Zero downtime upgrades for Crowd are currently not supported.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#jira-rolling-upgrade","title":"Jira rolling upgrade","text":"<p>Let's say we have Jira version <code>8.19.0</code> deployed to our Kubernetes cluster, and we want to upgrade it to version <code>8.19.1</code>, which we'll call the target version. You can substitute the target version for the one you need, as long as it's newer than the current one.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#1-find-tag-of-the-target-image","title":"1. Find tag of the target image.","text":"<p>Go to atlassian/jira-software Docker Hub page to pick a tag that matches your target version.</p> <p>In the example we're running Jira using the <code>8.19.0-jdk11</code> tag, and we'll be upgrading to <code>8.19.1-jdk11</code> - our target.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#2-put-jira-into-upgrade-mode","title":"2. Put Jira into upgrade mode.","text":"<p>Go to Administration &gt; Applications &gt; Jira upgrades and click Put Jira into upgrade mode.</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#3-run-the-upgrade-using-helm","title":"3. Run the upgrade using Helm.","text":"<p>Run the Helm upgrade command with your release name (<code>&lt;release-name&gt;</code>) and the target image from a previous step (<code>&lt;target-tag&gt;</code>). For more details, refer to the Helm documentation.</p> <pre><code>helm upgrade &lt;release-name&gt;  atlassian-data-center/jira \\ \n--wait \\\n--reuse-values \\\n--set image.tag=&lt;target-tag&gt;\n</code></pre> <p>If you used <code>kubectl scale</code> after installing the Helm chart, you'll need to add <code>--set replicaCount=&lt;number-of-jira-nodes&gt;</code> to the command. Otherwise, the deployment will be scaled back to the original number, which most likely is one node.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#4-wait-for-the-upgrade-to-finish","title":"4. Wait for the upgrade to finish.","text":"<p>The pods will be recreated with the updated version, one at a time.</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#5-finalize-the-upgrade","title":"5. Finalize the upgrade.","text":"<p>After all pods are active with the new version, click Run upgrade tasks to finalize the upgrade:</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#confluence-rolling-upgrade","title":"Confluence rolling upgrade","text":"<p>Let's say we have Confluence version <code>7.12.0</code> deployed to our Kubernetes cluster, and we want to upgrade it to version <code>7.12.1</code>, which we'll call the target version. You can substitute the target version for the one you need, as long as it's newer than the current one.</p> <p>Follow the link to confirm the target version is zero-downtime compatible<p>Upgrading Confluence with zero downtime </p> </p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#1-find-the-tag-of-the-target-image_1","title":"1. Find the tag of the target image.","text":"<p>Go to atlassian/confluence-server Docker Hub page to pick a tag that matches your target version.</p> <p>In the example we're running Confluence using the <code>7.12.0-jdk11</code> tag, and we'll be upgrading to <code>7.12.1-jdk11</code> - our target.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#2-put-confluence-into-upgrade-mode","title":"2. Put Confluence into upgrade mode.","text":"<p>From the admin page click on Rolling Upgrade and set the Confluence in Upgrade mode:</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#3-run-the-upgrade-using-helm_1","title":"3. Run the upgrade using Helm.","text":"<p>Run the Helm upgrade command with your release name (<code>&lt;release-name&gt;</code>) and the target image from a previous step (<code>&lt;target-tag&gt;</code>). For more details, refer to the Helm documentation.</p> <pre><code>helm upgrade &lt;release-name&gt;  atlassian-data-center/confluence \\\n--wait \\\n--reuse-values \\ \n--set image.tag=&lt;target-tag&gt;\n</code></pre> <p>If you used <code>kubectl scale</code> after installing the Helm chart, you'll need to add <code>--set replicaCount=&lt;number-of-confluence-nodes&gt;</code> to the command. Otherwise, the deployment will be scaled back to the original number, which most likely is one node.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#4-wait-for-the-upgrade-to-finish_1","title":"4. Wait for the upgrade to finish.","text":"<p>The pods will be recreated with the updated version, one at a time.</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#5-finalize-the-upgrade_1","title":"5. Finalize the upgrade.","text":"<p>After all the pods are activated with the new version, finalize the upgrade:</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#bitbucket-rolling-upgrade","title":"Bitbucket rolling upgrade","text":"<p>Let's say we have Bitbucket version <code>7.12.0</code> deployed to our Kubernetes cluster, and we want to upgrade it to version <code>7.12.1</code>, which we'll call the target version. You can substitute the target version for the one you need, as long as it's newer than the current one.</p> <p>Follow the link to find out if the target version is zero-downtime compatible<p>Upgrading Bitbucket with zero downtime</p> </p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#1-find-tag-of-the-target-image_1","title":"1. Find tag of the target image.","text":"<p>Go to atlassian/bitbucket-server Docker Hub page to pick a tag that matches your target version.</p> <p>In the example we're running Bitbucket using the <code>7.12.0-jdk11</code> tag, and we'll be upgrading to <code>7.12.1-jdk11</code> - our target.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#2-put-bitbucket-into-upgrade-mode","title":"2. Put Bitbucket into upgrade mode.","text":"<p>From the admin page click on Rolling Upgrade and set the Bitbucket to Upgrade mode:</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#3-run-the-upgrade-using-helm_2","title":"3. Run the upgrade using Helm.","text":"<p>Run the Helm upgrade command with your release name (<code>&lt;release-name&gt;</code>) and the target image from a previous step (<code>&lt;target-tag&gt;</code>). For more details, consult the Helm documentation.</p> <pre><code>helm upgrade &lt;release-name&gt;  atlassian-data-center/bitbucket \\\n--wait \\\n--reuse-values \\\n--set image.tag=&lt;target-tag&gt;\n</code></pre> <p>If you used <code>kubectl scale</code> after installing the Helm chart, you'll need to add <code>--set replicaCount=&lt;number-of-bb-nodes&gt;</code> to the command. Otherwise, the deployment will be scaled back to the original number, which most likely is one node.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#4-wait-for-the-upgrade-to-finish_2","title":"4. Wait for the upgrade to finish.","text":"<p>The pods will be recreated with the updated version, one at a time.</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#5-finalize-the-upgrade_2","title":"5. Finalize the upgrade.","text":"<p>After all the pods are active with the new version, finalize the upgrade:</p> <p></p>"}]}